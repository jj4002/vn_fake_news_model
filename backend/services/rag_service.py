# services/rag_service.py

from typing import Optional, Dict, List, Tuple
import logging
import os
import re

logger = logging.getLogger(__name__)

try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    logger.warning("sentence-transformers not available, RAG disabled")

from services.supabase_client import SupabaseService


# ============================================================
# HELPER CLASS 1: ContentChunker
# ============================================================

class ContentChunker:
    """Smart content chunker with sentence awareness."""

    @staticmethod
    def chunk_by_sentences(
        text: str,
        max_chunk_size: int = 300,
        overlap_ratio: float = 0.3,
    ) -> List[str]:
        """Chunk text by sentences with overlap."""
        sentences = re.split(r"[.!?]+\s+", text.strip())
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]

        if not sentences:
            return [text[:max_chunk_size]]

        chunks: List[str] = []
        current_chunk: List[str] = []
        current_len = 0
        overlap_size = int(max_chunk_size * overlap_ratio)

        for sentence in sentences:
            sentence_len = len(sentence)

            if current_len + sentence_len > max_chunk_size and current_chunk:
                # Save current chunk
                chunks.append(" ".join(current_chunk))

                # Build overlap chunk
                overlap_sentences: List[str] = []
                overlap_len = 0
                for s in reversed(current_chunk):
                    if overlap_len + len(s) <= overlap_size:
                        overlap_sentences.insert(0, s)
                        overlap_len += len(s)
                    else:
                        break

                current_chunk = overlap_sentences
                current_len = overlap_len

            current_chunk.append(sentence)
            current_len += sentence_len

        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks

    @staticmethod
    def extract_key_sentences(text: str, top_k: int = 3) -> str:
        """Extract key sentences (first + longest + last)."""
        sentences = re.split(r"[.!?]+\s+", text.strip())
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]

        if not sentences:
            return text[:300]

        if len(sentences) <= top_k:
            return " ".join(sentences)

        key_sentences: List[str] = [sentences[0]]

        # Longest middle sentence
        if len(sentences) > 2:
            longest = max(sentences[1:-1], key=len)
        else:
            longest = sentences[-1]
        if longest not in key_sentences:
            key_sentences.append(longest)

        # Last sentence
        if sentences[-1] not in key_sentences:
            key_sentences.append(sentences[-1])

        return " ".join(key_sentences[:top_k])


# ============================================================
# HELPER CLASS 2: QueryGenerator
# ============================================================

class QueryGenerator:
    """Generate multiple search queries for better coverage."""

    def __init__(self, chunker: ContentChunker):
        self.chunker = chunker

    def generate_queries(
        self,
        title: str,
        content: str,
    ) -> List[Tuple[str, float]]:
        """
        Generate multiple queries with weights.

        Returns:
            List of (query_text, weight) tuples.
        """
        queries: List[Tuple[str, float]] = []

        # Query 1: Title only (weight 1.0)
        if title and len(title.strip()) > 10:
            queries.append((title.strip(), 1.0))

        # Query 2: Title + key content (weight 1.3 - most important)
        if content:
            key_sentences = self.chunker.extract_key_sentences(content, top_k=2)
            combined = f"{title} {key_sentences}"[:500]
            if len(combined.strip()) > 10:
                queries.append((combined, 1.3))

        return queries if queries else [(title or content, 1.0)]


# ============================================================
# HELPER CLASS 3: AdaptiveThreshold
# ============================================================

class AdaptiveThreshold:
    """Calculate adaptive thresholds based on text length."""

    @staticmethod
    def calculate_threshold(title_length: int, content_length: int) -> Tuple[float, float]:
        """
        Calculate adaptive thresholds.

        Logic:
        - Short text (<250): Lower threshold (easier to find)
        - Long text (>1000): Higher threshold (more strict)
        - Normal text: Base threshold

        Returns:
            (search_threshold, verify_threshold)
        """
        base_search = 0.72      # Base search threshold (increased from 0.70)
        base_verify = 0.87      # Base verify threshold (increased from 0.85)

        total_len = title_length + content_length

        # Adjust based on length
        if total_len < 250:
            len_adj = -0.02     # Shorter text ‚Üí easier to match
        elif total_len > 1000:
            len_adj = 0.02      # Longer text ‚Üí stricter
        else:
            len_adj = 0.0       # Normal

        # Calculate final thresholds with bounds
        search_th = max(0.68, min(0.75, base_search + len_adj))
        verify_th = max(0.83, min(0.92, base_verify + len_adj))

        return search_th, verify_th


# ============================================================
# HELPER CLASS 4: ResultReranker
# ============================================================

class ResultReranker:
    """Re-rank results by similarity √ó source credibility."""

    SOURCE_SCORES = {
        "vnexpress": 1.2,
        "vtv": 1.2,
        "vov": 1.15,
        "60giay": 1.15,
        "thuvienphapluat": 1.15,
        "tuoitre": 1.1,
        "thanhnien": 1.1,
        "dantri": 1.1,
        "vietnamplus": 1.1,
        "zingnews": 1.05,
    }

    @classmethod
    def rerank(cls, results: List[Dict]) -> List[Dict]:
        """
        Re-rank by similarity √ó source credibility.

        Note: Uses ORIGINAL similarity, not weighted_score.
        """
        if not results:
            return []

        # Dedup by id/title, keep highest similarity
        seen: Dict[str, Dict] = {}
        for r in results:
            article_id = r.get("id") or r.get("title")
            sim = r.get("similarity", 0.0)

            if article_id in seen:
                if sim > seen[article_id]["similarity"]:
                    seen[article_id] = r
            else:
                seen[article_id] = r

        unique = list(seen.values())

        # Calculate final score = similarity √ó source multiplier
        for r in unique:
            sim = r.get("similarity", 0.0)
            source = (r.get("source") or "").lower()

            # Find source multiplier
            mult = 1.0
            for source_key, score in cls.SOURCE_SCORES.items():
                if source_key in source:
                    mult = score
                    break

            r["source_multiplier"] = mult
            r["final_score"] = sim * mult

        # Sort by final score
        unique.sort(key=lambda x: x.get("final_score", 0.0), reverse=True)
        return unique


# ============================================================
# MAIN CLASS: RAGService
# ============================================================

class RAGService:
    """Retrieval-Augmented Generation service for fact verification."""

    def __init__(self):
        self.supabase = SupabaseService()
        self.available = EMBEDDINGS_AVAILABLE

        if self.available:
            try:
                model_name = os.getenv(
                    "EMBEDDING_MODEL", "keepitreal/vietnamese-sbert"
                )
                self.embedding_model = SentenceTransformer(model_name)
                logger.info(f"‚úÖ RAG enabled with {model_name}")
            except Exception as e:
                logger.error(f"Failed to load embedding model: {e}")
                self.available = False
        else:
            self.embedding_model = None
            logger.warning("RAG service disabled")

        # Helper components
        self.chunker = ContentChunker()
        self.query_generator = QueryGenerator(self.chunker)
        self.threshold_calculator = AdaptiveThreshold()
        self.reranker = ResultReranker()

    # ==================== SHOULD_USE_RAG ====================
    def should_use_rag(
        self,
        title: str,
        content: str,
        base_confidence: float,
        author_id: str = "",
    ) -> bool:
        """
        Decide whether to use RAG based on CONTENT SENSITIVITY.

        Changes:
        - ‚ùå REMOVED: High confidence (>0.95) trigger
        - ‚úÖ ADDED: Arrest & legal keywords
        - ‚ùå REMOVED: Unknown source rule
        """
        if not self.available:
            return False

        text_lower = (title + " " + content).lower()

        # Rule 1: Clickbait patterns
        clickbait_keywords = [
            "ngay l·∫≠p t·ª©c", "kh·∫©n", "n√≥ng", "s·ªëc", "ch·∫•n ƒë·ªông",
            "tr∆∞·ªõc", "b·ªã ph·∫°t", "ti·ªÅn tri·ªáu", "h√£y v√†o",
            "ki·ªÉm tra ngay", "c·∫£nh b√°o", "b√≠ m·∫≠t",
        ]
        clickbait_count = sum(1 for kw in clickbait_keywords if kw in text_lower)

        if clickbait_count >= 3:
            logger.info("RAG triggered: Clickbait detected (%d keywords)", clickbait_count)
            return True

        # Rule 2: Sensitive topics (EXPANDED with arrest keywords)
        sensitive_keywords = [
            # ‚≠ê Arrest & Legal
            "b·ªã b·∫Øt", "b·∫Øt gi·ªØ", "ƒëi·ªÅu tra", "c√¥ng an", "kh·ªüi t·ªë",
            "t·∫°m giam", "kh√°m x√©t", "truy n√£", "giam gi·ªØ", "b·∫Øt t·∫°m giam",
            "c∆° quan ƒëi·ªÅu tra", "b·ªô c√¥ng an", "c√¥ng an tp", "b·ªã t·∫°m gi·ªØ",
            "ra t√≤a", "khai b√°o", "l√†m vi·ªác v·ªõi c√¥ng an",

            # T√†i ch√≠nh
            "t·∫∑ng ti·ªÅn", "ph√°t ti·ªÅn", "h·ªó tr·ª£ ti·ªÅn m·∫∑t", "nh·∫≠n ti·ªÅn",
            "b·ªô t√†i ch√≠nh ph√°t", "ch√≠nh ph·ªß t·∫∑ng", "m·ªói ng∆∞·ªùi d√¢n",
            "tr·ª£ c·∫•p", "ti·ªÅn h·ªó tr·ª£", "ti·ªÅn th∆∞·ªüng",

            # Y t·∫ø
            "virus m·ªõi", "b·ªánh l·∫°", "d·ªãch b·ªánh", "b√πng ph√°t",
            "vaccine nguy hi·ªÉm", "v√¥ sinh", "ch·∫øt sau ti√™m",
            "thu·ªëc ch·ªØa kh·ªèi", "b√≠ quy·∫øt ch·ªØa", "ph∆∞∆°ng ph√°p th·∫ßn k·ª≥",
            "tay ch√¢n mi·ªáng", "covid", "c√∫m", "ung th∆∞",

            # Ch√≠nh tr·ªã
            "b·ªô tr∆∞·ªüng", "t·ªïng b√≠ th∆∞", "th·ªß t∆∞·ªõng", "ch·ªß t·ªãch",
            "t·ª´ ch·ª©c", "b√™ b·ªëi", "tham nh≈©ng", "b√≠ m·∫≠t",
            "thu h·ªìi ƒë·∫•t", "c∆∞·ª°ng ch·∫ø", "bi·ªÉu t√¨nh",

            # Thi√™n tai
            "ƒë·ªông ƒë·∫•t", "l≈© l·ª•t", "ng·∫≠p l·ª•t", "s√≥ng th·∫ßn",
            "b√£o l·ªõn", "si√™u b√£o", "th·∫£m h·ªça", "s·∫°t l·ªü",
            "h√†ng trƒÉm ng∆∞·ªùi ch·∫øt", "h√†ng ng√†n n·∫°n nh√¢n",

            # Kinh t·∫ø
            "ng√¢n h√†ng s·ª•p ƒë·ªï", "ph√° s·∫£n", "v·ª° n·ª£", "r√∫t ti·ªÅn",
            "bitcoin tƒÉng", "ƒë·∫ßu t∆∞ ngay", "l√†m gi√†u nhanh",
            "c·ªï phi·∫øu s·ª•p", "kh·ªßng ho·∫£ng t√†i ch√≠nh",

            # Gi√°o d·ª•c
            "b·ªè thi", "mi·ªÖn h·ªçc ph√≠", "thay ƒë·ªïi quy ƒë·ªãnh",
            "t·ªët nghi·ªáp thpt", "ƒëi·ªÉm chu·∫©n", "x√©t tuy·ªÉn",
            "b·ªô gi√°o d·ª•c", "c·∫•m h·ªçc sinh",

            # X√£ h·ªôi
            "b·∫Øt c√≥c tr·∫ª em", "x√¢m h·∫°i", "c∆∞·ªõp gi·∫≠t", "ng∆∞·ªùi l·∫°",
            "nghi ph·∫°m", "t·ªôi ph·∫°m", "m·∫•t t√≠ch", "n·∫°n nh√¢n", "ƒë√°nh ng∆∞·ªùi",

            # Ch√≠nh ph·ªß & c∆° quan
            "vneid", "ch√≠nh ph·ªß", "b·ªô", "th√¥ng b√°o ch√≠nh th·ª©c",
            "quy ƒë·ªãnh", "lu·∫≠t", "bhyt", "bhxh", "cccd",
        ]

        if any(kw in text_lower for kw in sensitive_keywords):
            logger.info("RAG triggered: Sensitive topic detected")
            return True

        # Rule 3: Breaking news
        breaking_keywords = [
            "kh·∫©n c·∫•p", "v·ª´a xong", "breaking", "m·ªõi nh·∫•t", "tin n√≥ng",
            "ngay l√∫c n√†y", "ngay b√¢y gi·ªù", "hi·ªán t·∫°i", "ƒëang di·ªÖn ra",
            "v·ª´a m·ªõi", "ph√∫t tr∆∞·ªõc", "gi·ªù tr∆∞·ªõc", "h√¥m nay",
            "c·∫ßn ngay", "l·∫≠p t·ª©c", "g·∫•p", "h·ªèa t·ªëc", "kh·∫©n",
            "c·∫£nh b√°o kh·∫©n", "th√¥ng b√°o kh·∫©n", "b√°o ƒë·ªông",
            "tai n·∫°n", "th·∫£m h·ªça", "nguy hi·ªÉm", "c·∫≠p nh·∫≠t m·ªõi nh·∫•t",
            "tin m·ªõi", "v·ª´a ph√°t hi·ªán", "th√¥ng tin m·ªõi", "di·ªÖn bi·∫øn m·ªõi",
        ]

        if any(kw in text_lower for kw in breaking_keywords):
            logger.info("RAG triggered: Breaking news")
            return True

        logger.info("RAG not triggered: Normal content")
        return False

    # ==================== VERIFY (MAIN METHOD) ====================
    def verify(
        self,
        title: str,
        content: str,
        top_k: int = 5,
    ) -> Dict:
        """
        Verify content against news corpus.

        Simplified logic:
        1. Generate multiple queries
        2. Calculate adaptive thresholds
        3. Search all queries (keep original similarity)
        4. Re-rank by similarity √ó source credibility
        5. Use best match for decision

        Returns:
            {
                "recommendation": "VERIFIED" | "NEEDS_REVIEW" | "NO_RELIABLE_SOURCE_FOUND",
                "similarity_score": float,
                "matching_articles": List[Dict]
            }
        """
        if not self.available:
            return self._empty_result()

        try:
            # 1. Generate queries
            queries = self.query_generator.generate_queries(title, content)
            logger.info("üîç Generated %d RAG queries", len(queries))

            # 2. Calculate adaptive thresholds
            search_th, verify_th = self.threshold_calculator.calculate_threshold(
                len(title), len(content)
            )
            logger.info("üìä Thresholds: search=%.2f, verify=%.2f", search_th, verify_th)

            # 3. Search all queries
            all_results: List[Dict] = []

            for query_text, weight in queries:
                logger.info("   Q(w=%.1f): %s...", weight, query_text[:60])

                # Encode query
                query_embedding = self.embedding_model.encode(
                    query_text,
                    normalize_embeddings=True,
                )

                # Search in database
                results = self.supabase.search_similar_news(
                    query_embedding,
                    top_k=top_k * 2,  # Get more for dedup
                    threshold=search_th,
                )

                # Add query weight but keep original similarity
                for r in results:
                    r["query_weight"] = weight
                    r["weighted_score"] = r.get("similarity", 0.0) * weight

                all_results.extend(results)

            if not all_results:
                logger.info("‚ùå No matching articles found")
                return self._empty_result()

            # 4. Re-rank by similarity √ó source credibility
            ranked = self.reranker.rerank(all_results)
            logger.info("‚úÖ %d articles after rerank", len(ranked))

            # 5. Use best match
            best = ranked[0]
            similarity = best.get("similarity", 0.0)  # Original similarity

            logger.info(
                "   Best: %s (sim=%.2f, mult=%.2f)",
                best.get("source"),
                similarity,
                best.get("source_multiplier", 1.0),
            )

            # 6. Make recommendation
            if similarity >= verify_th:
                recommendation = "VERIFIED"
                logger.info("   ‚úÖ VERIFIED (‚â•%.2f)", verify_th)
            elif similarity >= search_th:
                recommendation = "NEEDS_REVIEW"
                logger.info("   ‚ö†Ô∏è NEEDS_REVIEW (%.2f ‚â§ sim < %.2f)", search_th, verify_th)
            else:
                recommendation = "NO_RELIABLE_SOURCE_FOUND"
                logger.info("   ‚ùå NO_RELIABLE_SOURCE (<%.2f)", search_th)

            return {
                "recommendation": recommendation,
                "similarity_score": similarity,
                "matching_articles": ranked[:3],
            }

        except Exception as e:
            logger.error("‚ùå RAG verification error: %s", e, exc_info=True)
            return self._empty_result()

    # ==================== HELPER METHODS ====================
    def _empty_result(self) -> Dict:
        """Return empty result when no articles found."""
        return {
            "recommendation": "NO_RELIABLE_SOURCE_FOUND",
            "similarity_score": 0.0,
            "matching_articles": [],
        }

    def retrieve_context(
        self,
        title: str,
        content: str,
        top_k: int = 3,
    ) -> Optional[str]:
        """
        Retrieve similar news articles as context string.

        Returns:
            Context string or None if no articles found.
        """
        verification = self.verify(title, content, top_k)

        if not verification["matching_articles"]:
            return None

        context_parts: List[str] = []
        for article in verification["matching_articles"]:
            snippet = f"[{article['source']}] {article['title']}"
            context_parts.append(snippet)

        return " | ".join(context_parts)