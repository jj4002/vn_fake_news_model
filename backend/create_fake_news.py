"""
HARD FAKE GENERATOR - GROQ + SUPABASE
3-TIER FAKE DIFFICULTY â†’ Tá»I Æ¯U CHO PHOBERT TRAINING
"""

from groq import Groq
from supabase import create_client, Client
import pandas as pd
import time
from tqdm import tqdm
import random
import os
import json
from dotenv import load_dotenv

# ===========================
# CONFIG
# ===========================
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not GROQ_API_KEY:
    raise ValueError("âŒ GROQ_API_KEY not found! Get key: https://console.groq.com/keys")
if not SUPABASE_KEY:
    raise ValueError("âŒ SUPABASE_KEY not found in environment!")

groq_client = Groq(api_key=GROQ_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

TRUSTED_SOURCES = ["vnexpress", "tuoitre", "thanhnien", "laodong", "dantri", "thuvienphapluat"]

# ===========================
# 3-TIER FAKE TEMPLATES - Tá»I Æ¯U PHOBERT
# ===========================

# TIER 1: EASY FAKE (30%) - Model há»c dá»…, build confidence
FAKE_EASY = [
    """Táº¡o tin GIáº¢ Dá»„ PHÃT HIá»†N vá»›i Dáº¤U HIá»†U RÃ•:

YÃŠU Cáº¦U:
- PHÃ“NG Äáº I Cá»°C Äá»˜ 10-50 láº§n (30 tá»· â†’ 500 tá»·)
- DÃ¹ng tá»« CLICKBAIT: "NÃ“NG", "Sá»C", "Báº¤T NGá»œ", "CHÆ¯A Tá»ªNG CÃ“"
- THIáº¾U NGUá»’N TIN: "theo nguá»“n tin", "Ä‘Æ°á»£c biáº¿t", "tin Ä‘á»“n", "má»™t sá»‘ nguá»“n"
- Quote quÃ¡ dÃ i/chi tiáº¿t khÃ´ng thá»±c táº¿
- Deadline khÃ´ng há»£p lÃ½ ("Ã¡p dá»¥ng tá»« ngÃ y mai", "cÃ³ hiá»‡u lá»±c ngay")

TIN THáº¬T:
{article}

TIN GIáº¢ CLICKBAIT:""",

    """Táº¡o tin GIáº¢ vá»›i SAI LOGIC RÃ• RÃ€NG:

YÃŠU Cáº¦U:
- PhÃ³ng Ä‘áº¡i 20x+ sá»‘ liá»‡u
- MÃ¢u thuáº«n ná»™i bá»™ (tiÃªu Ä‘á» nÃ³i 100B, ná»™i dung nÃ³i 200B)
- Thá»i gian phi lÃ½ (chÃ­nh sÃ¡ch 2030 Ã¡p dá»¥ng tá»« 2024)
- KhÃ´ng cÃ³ trÃ­ch dáº«n cá»¥ thá»ƒ tá»« quan chá»©c

TIN THáº¬T:
{article}

TIN GIáº¢ SAI LOGIC:""",
]

# TIER 2: MEDIUM FAKE (50%) - ThÃ¡ch thá»©c vá»«a pháº£i
FAKE_MEDIUM = [
    """Táº¡o tin GIáº¢ Vá»ªA PHáº¢I vá»›i Dáº¤U HIá»†U TINH VI:

YÃŠU Cáº¦U:
- PhÃ³ng Ä‘áº¡i 4-7 láº§n
- Style chuyÃªn nghiá»‡p NHÆ¯NG cÃ³ 2-3 hint nhá»:
  + Thiáº¿u nguá»“n tin chÃ­nh thá»‘ng (khÃ´ng nÃªu tÃªn Bá»™/Sá»Ÿ cá»¥ thá»ƒ)
  + DÃ¹ng "má»™t sá»‘ chuyÃªn gia", "theo Ä‘Ã¡nh giÃ¡" (mÆ¡ há»“)
  + Quote hÆ¡i quÃ¡ chi tiáº¿t/hoÃ n háº£o
- ThÃªm xáº¿p háº¡ng quá»‘c táº¿ KHÃ”NG XÃC MINH: "Top 10 chÃ¢u Ã theo Forbes"
- Timeline hÆ¡i vá»™i (Ä‘á» xuáº¥t 2026 â†’ Ã¡p dá»¥ng 2025)

TIN THáº¬T:
{article}

TIN GIáº¢ MEDIUM:""",

    """Táº¡o tin GIáº¢ BÃ“P MÃ‰O chÃ­nh sÃ¡ch Vá»ªA PHáº¢I:

YÃŠU Cáº¦U:
- Äá»•i Gáº¦N Háº¾T Ã½ chÃ­nh sÃ¡ch (khÃ´ng chá»‰ 1 tá»«):
  + "ThÃ­ Ä‘iá»ƒm 3 tá»‰nh" â†’ "Ãp dá»¥ng toÃ n quá»‘c báº¯t buá»™c"
  + "Äá» xuáº¥t giáº£m 10%" â†’ "ChÃ­nh thá»©c giáº£m 30%"
- Giá»¯ style nghiÃªm tÃºc nhÆ°ng thÃªm:
  + NgÃ y thÃ¡ng cá»¥ thá»ƒ quÃ¡ (khÃ´ng cÃ³ trong tin gá»‘c)
  + Pháº¡t/Quyá»n lá»£i khÃ´ng Ä‘Æ°á»£c nháº¯c trong tin tháº­t

TIN THáº¬T:
{article}

TIN GIáº¢ POLICY TWIST:""",

    """Táº¡o tin GIáº¢ "NGUá»’N QUá»C Táº¾" vá»›i Dáº¤U HIá»†U:

YÃŠU Cáº¦U:
- Má»Ÿ Ä‘áº§u: "Theo Forbes/Bloomberg/Reuters..."
- PhÃ³ng Ä‘áº¡i 5-8x vá» táº§m quan trá»ng
- ThÃªm báº£ng xáº¿p háº¡ng KHÃ”NG CÃ“ THáº¬T
- Style quá»‘c táº¿ NHÆ¯NG:
  + KhÃ´ng cÃ³ link nguá»“n cá»¥ thá»ƒ
  + KhÃ´ng nÃªu tÃªn tÃ¡c giáº£/ngÃ y Ä‘Äƒng
  + Dá»‹ch thuáº­t hÆ¡i stiff (dáº¥u hiá»‡u dá»‹ch mÃ¡y)

TIN THáº¬T:
{article}

TIN GIáº¢ INTERNATIONAL:""",
]

# TIER 3: HARD FAKE (20%) - Tinh vi, thá»­ thÃ¡ch model
FAKE_HARD = [
    """Táº¡o tin GIáº¢ Cá»°C Ká»² TINH VI, gáº§n nhÆ° KHÃ”NG THá»‚ PHÃ‚N BIá»†T:

YÃŠU Cáº¦U:
- PhÃ³ng Ä‘áº¡i CHá»ˆ 2-3 láº§n (Ä‘á»§ sai nhÆ°ng nghe há»£p lÃ½)
- Style 100% GIá»NG bÃ¡o lá»›n: VnExpress, Tuá»•i Tráº»
- Giá»¯ cáº¥u trÃºc: Lead â†’ Body â†’ Quote â†’ Káº¿t
- Sai CHÃNH XÃC 1 ÄIá»‚M quan trá»ng:
  + Sá» LIá»†U (30 tá»· â†’ 90 tá»·)
  + THá»œI GIAN (2026 â†’ 2025)
  + PHáº M VI (3 tá»‰nh â†’ toÃ n quá»‘c)
- KHÃ”NG Ä‘Æ°á»£c clickbait
- KHÃ”NG Ä‘Æ°á»£c thiáº¿u nguá»“n tin (pháº£i cÃ³ tÃªn Bá»™/cÆ¡ quan)

Má»¥c tiÃªu: CHá»ˆ FACT-CHECK Ká»¸ Má»šI PHÃT HIá»†N Ä‘Æ°á»£c!

TIN THáº¬T:
{article}

TIN GIáº¢ SIÃŠU TINH VI:""",

    """Táº¡o tin GIáº¢ "DÆ¯Æ NG ÄÃ”NG KÃCH TÃ‚Y" cá»±c tinh vi:

YÃŠU Cáº¦U:
- Giá»¯ 80% ná»™i dung ÄÃšNG
- THAY Äá»”I TRá»ŒNG TÃ‚M má»™t cÃ¡ch TINH Táº¾:
  + Ã kiáº¿n chuyÃªn gia â†’ Quyáº¿t Ä‘á»‹nh chÃ­nh thá»©c
  + Kiáº¿n nghá»‹ â†’ ChÃ­nh sÃ¡ch Ä‘Ã£ thÃ´ng qua
  + "Äang nghiÃªn cá»©u" â†’ "Sáº½ Ã¡p dá»¥ng"
- TiÃªu Ä‘á» lÃ¡i hiá»ƒu nháº§m nhÆ°ng CÃ“ THá»‚ GIáº¢I THÃCH Ä‘Æ°á»£c
- Style 100% bÃ¡o lá»›n, KHÃ”NG CÃ“ dáº¥u hiá»‡u rÃµ rÃ ng

TIN THáº¬T:
{article}

TIN GIáº¢ MISLEADING:""",
]

# ===========================
# WEIGHTED SAMPLING
# ===========================
def get_random_template():
    """Random template theo distribution tá»‘i Æ°u PhoBERT"""
    rand = random.random()
    
    if rand < 0.3:  # 30% easy
        return random.choice(FAKE_EASY)
    elif rand < 0.8:  # 50% medium
        return random.choice(FAKE_MEDIUM)
    else:  # 20% hard
        return random.choice(FAKE_HARD)

# ===========================
# FUNCTIONS
# ===========================
def get_real_articles(limit=500):
    """Láº¥y real news tá»« Supabase"""
    print("ğŸ“¡ Fetching real articles from Supabase...")
    
    response = supabase.table("news_corpus").select(
        "title, content, source"
    ).in_(
        "source", TRUSTED_SOURCES
    ).gte(
        "published_date", "2024-01-01"
    ).not_.is_(
        "embedding", "null"
    ).limit(limit).execute()
    
    articles = []
    for row in response.data:
        title = (row.get('title') or "").strip()
        content = (row.get('content') or "").strip()
        
        if len(title + content) > 200:
            articles.append({
                "title": title,
                "content": content[:2000],
                "source": row.get('source')
            })
    
    print(f"âœ… Found {len(articles)} real articles")
    return articles

def generate_fake_groq(article: dict, template: str) -> dict:
    """Táº¡o 1 fake báº±ng GROQ"""
    try:
        prompt = template.format(
            article=f"TiÃªu Ä‘á»: {article['title']}\n\nNá»™i dung: {article['content'][:1500]}"
        )
        
        response = groq_client.chat.completions.create(
            # model="llama-3.3-70b-versatile",  # 500 cÃ¡i Ä‘áº§u dÃ¹ng cÃ¡i nÃ y
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "Báº¡n lÃ  chuyÃªn gia táº¡o tin giáº£. Táº¡o CHÃNH XÃC theo yÃªu cáº§u, Ä‘á»«ng thÃªm giáº£i thÃ­ch."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800,
            temperature=0.8  # âœ… TÄƒng lÃªn 0.8 Ä‘á»ƒ Ä‘a dáº¡ng hÆ¡n
        )
        
        fake_text = response.choices[0].message.content.strip()
        
        # Parse title/content
        if "\n\n" in fake_text:
            parts = fake_text.split("\n\n", 1)
            title = parts[0].replace("TiÃªu Ä‘á»:", "").replace("**", "").strip()[:150]
            content = parts[1].replace("Ná»™i dung:", "").strip()[:1800]
        else:
            lines = fake_text.split('\n')
            title = lines[0].strip()[:150]
            content = '\n'.join(lines[1:]).strip()[:1800]
        
        return {
            "title": title,
            "content": content,
            "label": "FAKE"
        }
        
    except Exception as e:
        print(f"âŒ GROQ error: {e}")
        return None

def generate_dataset(num_fakes=5000):
    """Main - Generate dataset with 3-tier difficulty"""
    print("ğŸ”¥ HARD FAKE GENERATOR - 3-TIER DIFFICULTY!\n")
    print("ğŸ“Š Distribution: 30% Easy, 50% Medium, 20% Hard")
    print("âš¡ FAST MODE: 2s/fake (30 req/min)")
    print(f"â±ï¸  Estimated time: {num_fakes * 2 / 3600:.1f} hours\n")
    
    checkpoint_file = "checkpoint.json"
    
    if os.path.exists(checkpoint_file):
        print("ğŸ“‚ Found checkpoint, resuming...")
        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        print(f"   Loaded {len(dataset)} articles from checkpoint\n")
    else:
        real_articles = get_real_articles(limit=500)
        if len(real_articles) == 0:
            print("âŒ No real articles found!")
            return
        
        dataset = []
        print("\nğŸ“‹ Adding REAL articles...")
        for article in real_articles:
            dataset.append({
                "title": article["title"],
                "content": article["content"],
                "label": "REAL"
            })
        
        with open("real_articles.json", 'w', encoding='utf-8') as f:
            json.dump(real_articles, f, ensure_ascii=False)
    
    if not os.path.exists("real_articles.json"):
        print("âŒ real_articles.json not found!")
        return
        
    with open("real_articles.json", 'r', encoding='utf-8') as f:
        real_articles = json.load(f)
    
    fake_count = sum(1 for d in dataset if d['label'] == 'FAKE')
    
    print(f"\nğŸ¯ Generating {num_fakes - fake_count} more hard fakes...")
    print(f"   Current: {fake_count}/{num_fakes} fakes\n")
    
    pbar = tqdm(initial=fake_count, total=num_fakes, desc="Generating")
    
    # âœ… Track distribution
    easy_count = medium_count = hard_count = 0
    
    while fake_count < num_fakes:
        article = random.choice(real_articles)
        template = get_random_template()  # âœ… 3-tier sampling
        
        # Track difficulty
        if template in FAKE_EASY:
            easy_count += 1
        elif template in FAKE_MEDIUM:
            medium_count += 1
        else:
            hard_count += 1
        
        fake = generate_fake_groq(article, template)
        
        if fake and len(fake["title"]) > 20 and len(fake["content"]) > 100:
            dataset.append(fake)
            fake_count += 1
            pbar.update(1)
            
            if fake_count % 100 == 0:
                with open(checkpoint_file, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False)
                print(f"\nğŸ’¾ Checkpoint: {fake_count} | Easy:{easy_count} Med:{medium_count} Hard:{hard_count}")
        
        time.sleep(2.0)
    
    pbar.close()
    
    # Stats
    real_count = sum(1 for d in dataset if d['label'] == 'REAL')
    fake_count = len(dataset) - real_count
    
    print(f"\nğŸ“Š DATASET STATS:")
    print(f"   Total:  {len(dataset):,}")
    print(f"   REAL:   {real_count:,} ({real_count/len(dataset)*100:.1f}%)")
    print(f"   FAKE:   {fake_count:,} ({fake_count/len(dataset)*100:.1f}%)")
    print(f"\nğŸ“Š FAKE DISTRIBUTION:")
    print(f"   Easy:   {easy_count} (~{easy_count/fake_count*100:.0f}%)")
    print(f"   Medium: {medium_count} (~{medium_count/fake_count*100:.0f}%)")
    print(f"   Hard:   {hard_count} (~{hard_count/fake_count*100:.0f}%)")
    
    random.shuffle(dataset)
    
    print("\nğŸ’¾ Saving final CSV...")
    df = pd.DataFrame(dataset)
    df.to_csv("fake_news_dataset.csv", index=False, encoding='utf-8-sig')
    print(f"   âœ… fake_news_dataset.csv ({len(df):,} rows)")
    
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)
    if os.path.exists("real_articles.json"):
        os.remove("real_articles.json")
    
    print("\nğŸ‰ DONE! 3-TIER DATASET FOR PHOBERT!")
    print("\nâœ… PhoBERT sáº½ há»c:")
    print("   - Easy cases: Build confidence, learn obvious patterns")
    print("   - Medium cases: Learn subtle hints & context")
    print("   - Hard cases: Deep fact-checking, sá»‘ liá»‡u chÃ­nh xÃ¡c")

if __name__ == "__main__":
    generate_dataset(5000)
