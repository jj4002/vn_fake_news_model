# -*- coding: utf-8 -*-
"""fakenews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19lG6MVwLCv7qvOTkliDWBYmpqnZE9q2f
"""

# Commented out IPython magic to ensure Python compatibility.
# Cell 1: Install dependencies (simplified)
# %pip install torch transformers datasets huggingface-hub tokenizers
# %pip install py_vncorenlp
# %pip install yt-dlp openai-whisper gradio underthesea
# %pip install ydata_profiling

print("✅ All packages installed successfully!")

import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import (
    RobertaForSequenceClassification,
    RobertaConfig,
    AutoTokenizer,
    get_linear_schedule_with_warmup
)
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# Check GPU
print(f"GPU available: {torch.cuda.is_available()}")
print(f"GPU name: {torch.cuda.get_device_name(0)}")

!unzip -q /content/Dataset.zip -d /content/

"""##Tìm hiểu tập dataset

Dataset 1: Vietnamese Fake News - https://github.com/hiepnguyenduc2005/Vietnamese-Fake-News-Detection/tree/main
"""

data1_train = pd.read_csv('train.csv')
data1_val = pd.read_csv('val.csv')
data1_test = pd.read_csv('test.csv')

data1_train['label'] = data1_train['label'].astype('category')

from ydata_profiling import ProfileReport

profile = ProfileReport(
    data1_train,
    title="Data 1 train Profile",
    explorative=True,
    correlations={
        "auto": {"calculate": True},
        "pearson": {"calculate": True},
        "spearman": {"calculate": True},
        "cramers": {"calculate": True},
    }
)
profile.to_file("profile_report_data_1_train.html")

# Tạo DataFrame mới chỉ với 2 cột: text và label
# Train set
train_clean = pd.DataFrame({
    'text': data1_train['post_message'],
    'label': data1_train['label']
})

# Validation set
val_clean = pd.DataFrame({
    'text': data1_val['post_message'],
    'label': data1_val['label']
})

# Test set (nếu test set có label, nếu không thì bỏ label)
test_clean = pd.DataFrame({
    'text': data1_test['post_message'],
    'label': data1_test['label']
})

# Lưu ra file CSV mới
train_clean.to_csv('train_clean.csv', index=False, encoding='utf-8')
val_clean.to_csv('val_clean.csv', index=False, encoding='utf-8')
test_clean.to_csv('test_clean.csv', index=False, encoding='utf-8')

# Kiểm tra kết quả
print("=== Train set ===")
print(f"Shape: {train_clean.shape}")
print(f"\n{train_clean.head()}")

print("\n=== Validation set ===")
print(f"Shape: {val_clean.shape}")
print(f"\n{val_clean.head()}")

print("\n=== Test set ===")
print(f"Shape: {test_clean.shape}")
print(f"\n{test_clean.head()}")

print("\n✓ Files saved successfully!")
print("  - train_clean.csv")
print("  - val_clean.csv")
print("  - test_clean.csv")

"""Dataset 2: Vietnamese Fake News - http://github.com/WhySchools/VFND-vietnamese-fake-news-datasets"""

data2_train = pd.read_csv('vn_news_226_tlfr.csv')

data2_train['label'] = data2_train['label'].astype('category')

profile = ProfileReport(
    data2_train,
    title="Data 2 train Profile",
    explorative=True,
    correlations={
        "auto": {"calculate": True},
        "pearson": {"calculate": True},
        "spearman": {"calculate": True},
        "cramers": {"calculate": True},
    }
)
profile.to_file("profile_report_data_2_train.html")

# 1. Load dataset 1 (từ file gốc với nhiều cột)
data1_train_raw = pd.read_csv('train.csv')

# Tạo clean version với 2 cột text, label
data1_train = pd.DataFrame({
    'text': data1_train_raw['post_message'],
    'label': data1_train_raw['label']
})

# 2. Load dataset 2 (đã có format text, label)
data2_train = pd.read_csv('vn_news_226_tlfr.csv')

# Kiểm tra tên cột của data2_train
print("=== Data2 columns ===")
print(data2_train.columns.tolist())
print(f"Shape: {data2_train.shape}")

# Đảm bảo cột label là số nguyên (0/1)
data2_train['label'] = data2_train['label'].astype(int)
data1_train['label'] = data1_train['label'].astype(int)

# 3. Kiểm tra duplicates TRƯỚC khi merge
print("\n=== Duplicates in data1_train ===")
print(f"Total rows: {len(data1_train)}")
print(f"Duplicate rows: {data1_train.duplicated().sum()}")

print("\n=== Duplicates in data2_train ===")
print(f"Total rows: {len(data2_train)}")
print(f"Duplicate rows: {data2_train.duplicated().sum()}")

# 4. Xóa duplicates trong từng dataset trước
data1_train_clean = data1_train.drop_duplicates(subset=['text'], keep='first')
data2_train_clean = data2_train.drop_duplicates(subset=['text'], keep='first')

print(f"\n=== After removing duplicates ===")
print(f"data1_train: {len(data1_train)} -> {len(data1_train_clean)} rows")
print(f"data2_train: {len(data2_train)} -> {len(data2_train_clean)} rows")

# 5. Ghép 2 datasets
combined_train = pd.concat([data1_train_clean, data2_train_clean], ignore_index=True)

print(f"\n=== After concatenation ===")
print(f"Combined shape: {combined_train.shape}")
print(f"Total duplicates: {combined_train.duplicated(subset=['text']).sum()}")

# 6. Xóa duplicates sau khi ghép (nếu còn)
combined_train_final = combined_train.drop_duplicates(subset=['text'], keep='first')

print(f"\n=== Final dataset ===")
print(f"Final shape: {combined_train_final.shape}")
print(f"Label distribution:")
print(combined_train_final['label'].value_counts())

# 8. Lưu file
combined_train_final.to_csv('combined_train.csv', index=False, encoding='utf-8')

print(f"\n✓ Final dataset saved!")
print(f"  - File: combined_train.csv")
print(f"  - Total samples: {len(combined_train_final)}")
print(f"  - Label 0: {(combined_train_final['label']==0).sum()}")
print(f"  - Label 1: {(combined_train_final['label']==1).sum()}")

# Xem mẫu dữ liệu
print("\n=== Sample data ===")
print(combined_train_final.head())

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# 1. Load data
train_df = pd.read_csv('combined_train.csv')
val_df = pd.read_csv('val_clean.csv')
test_df = pd.read_csv('test_clean.csv')

print(f"Train: {len(train_df)} samples")
print(f"Val: {len(val_df)} samples")
print(f"Test: {len(test_df)} samples")

# 2. Load PhoBERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base-v2")

# 3. Create Dataset class
class VietnameseNewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# 4. Create DataLoaders
train_dataset = VietnameseNewsDataset(
    train_df['text'].values,
    train_df['label'].values,
    tokenizer,
    max_length=256
)

val_dataset = VietnameseNewsDataset(
    val_df['text'].values,
    val_df['label'].values,
    tokenizer,
    max_length=256
)

test_dataset = VietnameseNewsDataset(
    test_df['text'].values,
    test_df['label'].values,
    tokenizer,
    max_length=256
)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# 5. Load PhoBERT model for classification
model = RobertaForSequenceClassification.from_pretrained(
    "vinai/phobert-base-v2",
    num_labels=2,
    output_attentions=False,
    output_hidden_states=False
)
model.to(device)

# 6. Setup optimizer and scheduler
epochs = 5
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)

total_steps = len(train_loader) * epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

# 7. Training function
def train_epoch(model, data_loader, optimizer, scheduler, device):
    model.train()
    losses = []
    correct_predictions = 0
    total_predictions = 0

    progress_bar = tqdm(data_loader, desc='Training')

    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        logits = outputs.logits

        _, preds = torch.max(logits, dim=1)
        correct_predictions += torch.sum(preds == labels)
        total_predictions += labels.size(0)

        losses.append(loss.item())

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()

        progress_bar.set_postfix({
            'loss': np.mean(losses),
            'acc': correct_predictions.double().item() / total_predictions
        })

    return correct_predictions.double() / total_predictions, np.mean(losses)

# 8. Evaluation function
def eval_model(model, data_loader, device):
    model.eval()
    losses = []
    correct_predictions = 0
    total_predictions = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc='Evaluating'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss
            logits = outputs.logits

            _, preds = torch.max(logits, dim=1)

            correct_predictions += torch.sum(preds == labels)
            total_predictions += labels.size(0)
            losses.append(loss.item())

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = correct_predictions.double() / total_predictions
    f1 = f1_score(all_labels, all_preds, average='weighted')

    return accuracy, np.mean(losses), f1, all_preds, all_labels

# 9. Training loop
print("\n=== Starting Training ===")
best_val_acc = 0

for epoch in range(epochs):
    print(f'\nEpoch {epoch + 1}/{epochs}')
    print('-' * 50)

    train_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)
    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')

    val_acc, val_loss, val_f1, _, _ = eval_model(model, val_loader, device)
    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}')

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_phobert_fake_news.pt')
        print(f'✓ Saved best model with val_acc: {best_val_acc:.4f}')

# 10. Test evaluation
print("\n=== Testing on Test Set ===")
model.load_state_dict(torch.load('best_phobert_fake_news.pt'))
test_acc, test_loss, test_f1, test_preds, test_labels = eval_model(model, test_loader, device)

print(f'\nTest Loss: {test_loss:.4f}')
print(f'Test Accuracy: {test_acc:.4f}')
print(f'Test F1 Score: {test_f1:.4f}')

print('\n=== Classification Report ===')
print(classification_report(test_labels, test_preds, target_names=['Real', 'Fake']))

cm = confusion_matrix(test_labels, test_preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# 11. Save model
model.save_pretrained('./phobert_fake_news_model')
tokenizer.save_pretrained('./phobert_fake_news_model')
print("\n✓ Model saved to ./phobert_fake_news_model")