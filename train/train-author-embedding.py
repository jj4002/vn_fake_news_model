# -*- coding: utf-8 -*-
"""fakenew_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hQ5XJw-YxkuoWPttMcwVZwWngHFKp5Uy
"""

# Commented out IPython magic to ensure Python compatibility.
# Cell 1: Install dependencies (simplified)
# %pip install torch transformers datasets huggingface-hub tokenizers
# %pip install py_vncorenlp
# %pip install ydata_profiling

print("âœ… All packages installed successfully!")

import torch
from torch import nn
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoTokenizer, RobertaModel, get_linear_schedule_with_warmup
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_recall_fscore_support
from sklearn.preprocessing import LabelEncoder
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import gc
import pickle
import json
import os
import zipfile
from datetime import datetime

warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ðŸ”¥ Using device: {device}")

# ==================== 1. LOAD DATA ====================
print("\n" + "="*70)
print("ðŸ“Š LOADING DATA")
print("="*70)

train = pd.read_csv('final_train_stratified.csv')
val = pd.read_csv('final_val_stratified.csv')
test = pd.read_csv('final_test_stratified.csv')

print(f"âœ“ Train: {len(train)} samples")
print(f"âœ“ Val: {len(val)} samples")
print(f"âœ“ Test: {len(test)} samples")

# ==================== 2. ENCODE AUTHOR ====================
print("\nðŸ“Š Encoding authors...")

# Combine all authors to fit encoder
all_authors = pd.concat([train['author_id'], val['author_id'], test['author_id']])
author_encoder = LabelEncoder()
author_encoder.fit(all_authors)

# Encode
train['author_encoded'] = author_encoder.transform(train['author_id'])
val['author_encoded'] = author_encoder.transform(val['author_id'])
test['author_encoded'] = author_encoder.transform(test['author_id'])

num_authors = len(author_encoder.classes_)
print(f"âœ“ Total unique authors: {num_authors}")

# Check which are "unknown"
unknown_indices = [i for i, name in enumerate(author_encoder.classes_)
                   if 'unknown' in name.lower()]
print(f"âœ“ Unknown author indices: {unknown_indices}")

# ==================== 3. MODEL ARCHITECTURE ====================
class PhoBERTFakeNewsDetector(nn.Module):
    """
    PhoBERT + Author Embedding vá»›i Adaptive Gating

    âœ… Model sáº½ Tá»° Há»ŒC author nÃ o quan trá»ng (VTV24), author nÃ o khÃ´ng (unknown)
    """

    def __init__(self, num_authors, author_embed_dim=64, dropout_rate=0.3):
        super(PhoBERTFakeNewsDetector, self).__init__()

        # PhoBERT backbone
        self.phobert = RobertaModel.from_pretrained("vinai/phobert-base-v2")

        # âœ… Author embedding - Model sáº½ há»c embedding cho Tá»ªNG author
        # "unknown" sáº½ cÃ³ embedding gáº§n 0 (khÃ´ng quan trá»ng)
        # "VTV24" sáº½ cÃ³ embedding cÃ³ Ã½ nghÄ©a (quan trá»ng)
        self.author_embedding = nn.Embedding(num_authors, author_embed_dim)

        # âœ… Gating mechanism - Model tá»± há»c khi nÃ o tin author
        self.author_gate = nn.Sequential(
            nn.Linear(author_embed_dim, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()  # 0 = ignore, 1 = trust
        )

        # Text-only branch (cho Facebook/unknown author)
        self.text_branch = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 64),
            nn.ReLU()
        )

        # Combined branch (cho TikTok vá»›i valid author)
        self.combined_branch = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(768 + author_embed_dim, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 64),
            nn.ReLU()
        )

        # Final classifier
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(64, 2)
        )

    def forward(self, input_ids, attention_mask, author_ids,
                has_valid_author=None, training=True):
        batch_size = input_ids.size(0)

        # Get PhoBERT features
        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)
        text_features = outputs.last_hidden_state[:, 0, :]

        # Get author embedding
        author_embed = self.author_embedding(author_ids)

        # Calculate gate value
        gate = self.author_gate(author_embed)

        # âœ… FIX: Apply constraint ALWAYS (not just during training)
        if has_valid_author is not None:
            # Mask out invalid authors (set their gate to near 0)
            valid_mask = has_valid_author.unsqueeze(1).float()
            gate = gate * valid_mask + 0.05 * (1 - valid_mask)

        # Process through branches
        text_out = self.text_branch(text_features)
        combined_in = torch.cat([text_features, author_embed], dim=1)
        combined_out = self.combined_branch(combined_in)

        # Adaptive fusion
        features = (1 - gate) * text_out + gate * combined_out

        # Final prediction
        logits = self.classifier(features)

        return logits, gate

# ==================== 4. DATASET ====================
class FakeNewsDataset(Dataset):
    def __init__(self, texts, authors, labels, has_valid_author, tokenizer, max_length=256):
        self.texts = texts
        self.authors = authors
        self.labels = labels
        self.has_valid_author = has_valid_author
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            str(self.texts[idx]),
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'author_ids': torch.tensor(self.authors[idx], dtype=torch.long),
            'has_valid_author': torch.tensor(self.has_valid_author[idx], dtype=torch.bool),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

# ==================== 5. CREATE DATALOADERS ====================
print("\nðŸ“Š Creating dataloaders...")

tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base-v2")

train_dataset = FakeNewsDataset(
    train['text'].values, train['author_encoded'].values,
    train['label'].values, train['has_valid_author'].values, tokenizer
)
val_dataset = FakeNewsDataset(
    val['text'].values, val['author_encoded'].values,
    val['label'].values, val['has_valid_author'].values, tokenizer
)
test_dataset = FakeNewsDataset(
    test['text'].values, test['author_encoded'].values,
    test['label'].values, test['has_valid_author'].values, tokenizer
)

batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print(f"âœ“ Train batches: {len(train_loader)}")
print(f"âœ“ Val batches: {len(val_loader)}")
print(f"âœ“ Test batches: {len(test_loader)}")

# ==================== 6. INITIALIZE MODEL ====================
print("\nðŸ“Š Initializing model...")

model = PhoBERTFakeNewsDetector(num_authors=num_authors, author_embed_dim=64, dropout_rate=0.3)
model.to(device)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"âœ“ Total parameters: {total_params:,}")
print(f"âœ“ Trainable parameters: {trainable_params:,}")

# ==================== 7. TRAINING SETUP ====================
epochs = 8
learning_rate = 2e-5

# Optimizer with different learning rates
optimizer = AdamW([
    {'params': model.phobert.parameters(), 'lr': learning_rate * 0.1},
    {'params': model.author_embedding.parameters(), 'lr': learning_rate * 10},
    {'params': model.author_gate.parameters(), 'lr': learning_rate * 10},
    {'params': model.text_branch.parameters(), 'lr': learning_rate * 2},
    {'params': model.combined_branch.parameters(), 'lr': learning_rate * 2},
    {'params': model.classifier.parameters(), 'lr': learning_rate * 2}
], eps=1e-8, weight_decay=0.02)  # âœ… TÄƒng tá»« 0.01 â†’ 0.02

total_steps = len(train_loader) * epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps
)

# âœ… Better: TÄƒng weight cho Fake class
class WeightedFocalLoss(nn.Module):
    def __init__(self, alpha=0.7, gamma=2, label_smoothing=0.1):  # Add smoothing
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.label_smoothing = label_smoothing

    def forward(self, inputs, targets):
        # Smooth labels
        num_classes = inputs.size(1)
        smoothed_targets = targets * (1 - self.label_smoothing) + self.label_smoothing / num_classes

        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)
        pt = torch.exp(-ce_loss)
        alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)
        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

loss_fn = WeightedFocalLoss(alpha=0.7, gamma=2, label_smoothing=0.1)

# ==================== 8. TRAINING FUNCTIONS ====================
torch.backends.cudnn.benchmark = True
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

def train_epoch(model, data_loader, optimizer, scheduler, loss_fn, device, epoch=0, total_epochs=1):
    model.train()
    losses = []
    correct = 0
    total = 0

    pbar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{total_epochs}', leave=False, ncols=100)

    for batch_idx, batch in enumerate(pbar):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        author_ids = batch['author_ids'].to(device)
        has_valid = batch['has_valid_author'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        with autocast():
            logits, gates = model(input_ids, attention_mask, author_ids, has_valid, training=True)
            loss = loss_fn(logits, labels)

            # âœ… REDUCED regularization (0.1 â†’ 0.05)
            invalid_mask = ~has_valid
            if invalid_mask.any():
                gate_penalty = (gates[invalid_mask] ** 2).mean()
                loss = loss + 0.05 * gate_penalty

        # âœ… NaN detection
        if torch.isnan(loss):
            print(f"\nâš ï¸ NaN loss detected at batch {batch_idx}! Skipping...")
            continue

        scaler.scale(loss).backward()

        # âœ… Check for NaN gradients
        nan_grads = False
        for name, param in model.named_parameters():
            if param.grad is not None and torch.isnan(param.grad).any():
                print(f"\nâš ï¸ NaN gradient in {name}")
                nan_grads = True
                break

        if nan_grads:
            optimizer.zero_grad()
            continue

        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()

        _, preds = torch.max(logits, dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)
        losses.append(loss.item())

        pbar.set_postfix({'loss': f'{np.mean(losses):.4f}', 'acc': f'{correct/total:.4f}'})

    pbar.close()
    return correct / total, np.mean(losses)

def eval_model(model, data_loader, loss_fn, device, desc='Evaluating'):
    model.eval()
    losses = []
    all_preds = []
    all_labels = []
    all_gates = []

    with torch.no_grad():
        pbar = tqdm(data_loader, desc=desc, leave=False, ncols=100, dynamic_ncols=False)

        for batch in pbar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            author_ids = batch['author_ids'].to(device)
            has_valid = batch['has_valid_author'].to(device)
            labels = batch['labels'].to(device)

            logits, gates = model(input_ids, attention_mask, author_ids, has_valid, training=False)
            loss = loss_fn(logits, labels)

            _, preds = torch.max(logits, dim=1)

            losses.append(loss.item())
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_gates.extend(gates.cpu().numpy())

            pbar.set_postfix({'loss': f'{np.mean(losses):.4f}'})

        pbar.close()

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted')

    return accuracy, np.mean(losses), f1, all_preds, all_labels, all_gates

# ==================== 9. TRAINING LOOP ====================
print("\n" + "="*70)
print("ðŸš€ START TRAINING")
print("="*70)

best_val_f1 = 0  # âœ… Changed from best_val_acc
best_val_acc = 0  # Keep for logging
history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}

save_dir = './phobert_for_onnx'
os.makedirs(save_dir, exist_ok=True)

for epoch in range(epochs):
    print(f'\nðŸ“Š Epoch {epoch + 1}/{epochs}')
    print('-' * 50)

    train_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler, loss_fn, device, epoch=epoch, total_epochs=epochs)
    val_acc, val_loss, val_f1, _, _, val_gates = eval_model(model, val_loader, loss_fn, device, desc=f'Val Epoch {epoch+1}')

    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    history['val_f1'].append(val_f1)

    print(f'\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')
    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}')
    print(f'Avg Gate Value: {np.mean(val_gates):.4f}')

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

    # âœ… CHANGED: Save based on F1 instead of accuracy
    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        best_val_acc = val_acc  # Update for logging

        print(f'\nðŸ’¾ Saving model...')
        print(f'   Best F1: {best_val_f1:.4f} | Acc: {best_val_acc:.4f}')

        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc,
            'val_f1': val_f1,
            'num_authors': num_authors
        }, f'{save_dir}/best_model_weights.pt')

        config = {
            'model_name': 'PhoBERTFakeNewsDetector',
            'num_authors': num_authors,
            'author_embed_dim': 64,
            'dropout_rate': 0.2,
            'max_length': 256,
            'phobert_model': 'vinai/phobert-base-v2',
            'val_acc': float(val_acc),
            'val_f1': float(val_f1),
            'epoch': epoch,
            'best_metric': 'f1-score'  # âœ… Document what we optimized for
        }
        with open(f'{save_dir}/model_config.json', 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)

        with open(f'{save_dir}/author_encoder.pkl', 'wb') as f:
            pickle.dump(author_encoder, f)

        tokenizer.save_pretrained(f'{save_dir}/tokenizer')

        author_classes = {
            'classes': author_encoder.classes_.tolist(),
            'num_classes': len(author_encoder.classes_),
            'unknown_indices': [i for i, name in enumerate(author_encoder.classes_) if 'unknown' in name.lower()]
        }
        with open(f'{save_dir}/author_classes.json', 'w', encoding='utf-8') as f:
            json.dump(author_classes, f, indent=2, ensure_ascii=False)

        label_mapping = {'label_to_name': {0: 'Real', 1: 'Fake'}, 'name_to_label': {'Real': 0, 'Fake': 1}}
        with open(f'{save_dir}/label_mapping.json', 'w', encoding='utf-8') as f:
            json.dump(label_mapping, f, indent=2, ensure_ascii=False)

        with open(f'{save_dir}/training_history.json', 'w') as f:
            json.dump(history, f, indent=2)

        sample_data = {
            'text': test['text'].iloc[0],
            'author_id': test['author_id'].iloc[0],
            'label': int(test['label'].iloc[0]),
            'has_valid_author': bool(test['has_valid_author'].iloc[0])
        }
        with open(f'{save_dir}/sample_test.json', 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, indent=2, ensure_ascii=False)

        print(f'ðŸŽ¯ Saved! Best Val F1: {best_val_f1:.4f} (Acc: {best_val_acc:.4f})')

# âœ… Print final summary after training
print("\n" + "="*70)
print("âœ… TRAINING COMPLETED!")
print("="*70)
print(f"\nðŸ“Š Best Results:")
print(f"   Val F1-Score: {best_val_f1:.4f}")
print(f"   Val Accuracy: {best_val_acc:.4f}")
print(f"   Saved to: {save_dir}/")

# ==================== 10. TESTING ====================
print("\n" + "="*70)
print("ðŸ“Š FINAL EVALUATION ON TEST SET")
print("="*70)

checkpoint = torch.load(f'{save_dir}/best_model_weights.pt', map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])

test_acc, test_loss, test_f1, test_preds, test_labels, test_gates = eval_model(
    model, test_loader, loss_fn, device
)

print(f'\nðŸŽ¯ Test Results:')
print(f'  Accuracy: {test_acc:.4f}')
print(f'  F1 Score: {test_f1:.4f}')
print(f'  Loss: {test_loss:.4f}')

# ==================== ESSENTIAL VISUALIZATIONS ONLY ====================
print("\n" + "="*70)
print("ðŸ“Š CREATING ESSENTIAL VISUALIZATIONS")
print("="*70)

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ==================== 1. TRAINING HISTORY (CONVERGENCE CHECK) ====================
print("\nðŸ“ˆ Plotting training convergence...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

epochs_range = range(1, len(history['train_loss']) + 1)

# 1.1 Training & Validation Loss
ax = axes[0, 0]
ax.plot(epochs_range, history['train_loss'], 'o-', label='Train Loss',
        linewidth=2, markersize=8, color='#FF6B6B')
ax.plot(epochs_range, history['val_loss'], 's-', label='Val Loss',
        linewidth=2, markersize=8, color='#4ECDC4')
ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')
ax.set_ylabel('Loss', fontsize=12, fontweight='bold')
ax.set_title('Loss Convergence', fontsize=14, fontweight='bold', pad=10)
ax.legend(fontsize=11, loc='upper right')
ax.grid(alpha=0.3)

# Add final values
final_train_loss = history['train_loss'][-1]
final_val_loss = history['val_loss'][-1]
ax.text(0.02, 0.98, f'Final:\nTrain: {final_train_loss:.4f}\nVal: {final_val_loss:.4f}',
        transform=ax.transAxes, fontsize=10, verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# 1.2 Training & Validation Accuracy
ax = axes[0, 1]
ax.plot(epochs_range, history['train_acc'], 'o-', label='Train Acc',
        linewidth=2, markersize=8, color='#FF6B6B')
ax.plot(epochs_range, history['val_acc'], 's-', label='Val Acc',
        linewidth=2, markersize=8, color='#4ECDC4')
ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')
ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')
ax.set_title('Accuracy Convergence', fontsize=14, fontweight='bold', pad=10)
ax.legend(fontsize=11, loc='lower right')
ax.grid(alpha=0.3)

# Add final values
final_train_acc = history['train_acc'][-1]
final_val_acc = history['val_acc'][-1]
gap = (final_train_acc - final_val_acc) * 100
ax.text(0.02, 0.02, f'Final:\nTrain: {final_train_acc:.2%}\nVal: {final_val_acc:.2%}\nGap: {gap:.2f}%',
        transform=ax.transAxes, fontsize=10, verticalalignment='bottom',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# 1.3 Validation F1-Score
ax = axes[1, 0]
ax.plot(epochs_range, history['val_f1'], 'D-', label='Val F1-Score',
        linewidth=2, markersize=8, color='#95E1D3')
ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')
ax.set_ylabel('F1-Score', fontsize=12, fontweight='bold')
ax.set_title('F1-Score Progress', fontsize=14, fontweight='bold', pad=10)
ax.legend(fontsize=11, loc='lower right')
ax.grid(alpha=0.3)

# Mark best epoch
best_epoch = np.argmax(history['val_f1']) + 1
best_f1 = max(history['val_f1'])
ax.axvline(x=best_epoch, color='red', linestyle='--', linewidth=2, alpha=0.7)
ax.text(best_epoch, best_f1, f'  Best: {best_f1:.4f}\n  Epoch {best_epoch}',
        fontsize=10, verticalalignment='bottom',
        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))

# 1.4 Train-Val Gap Analysis
ax = axes[1, 1]
acc_gap = [(train - val) * 100 for train, val in zip(history['train_acc'], history['val_acc'])]
ax.plot(epochs_range, acc_gap, 'o-', linewidth=2, markersize=8, color='#F38181')
ax.axhline(y=3, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='3% Threshold')
ax.axhline(y=5, color='red', linestyle='--', linewidth=2, alpha=0.7, label='5% Overfitting')
ax.fill_between(epochs_range, 0, 3, alpha=0.2, color='green', label='Good Zone')
ax.fill_between(epochs_range, 3, 5, alpha=0.2, color='yellow', label='Caution Zone')
ax.fill_between(epochs_range, 5, 10, alpha=0.2, color='red', label='Overfitting Zone')
ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')
ax.set_ylabel('Train-Val Gap (%)', fontsize=12, fontweight='bold')
ax.set_title('Overfitting Analysis', fontsize=14, fontweight='bold', pad=10)
ax.legend(fontsize=9, loc='upper right')
ax.grid(alpha=0.3)
ax.set_ylim([0, max(max(acc_gap) + 1, 6)])

# Add final gap status
final_gap = acc_gap[-1]
status = 'âœ… Good' if final_gap < 3 else 'âš ï¸ Caution' if final_gap < 5 else 'âŒ Overfitting'
ax.text(0.02, 0.98, f'Final Gap: {final_gap:.2f}%\nStatus: {status}',
        transform=ax.transAxes, fontsize=11, verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.suptitle('Training Convergence & Overfitting Analysis',
             fontsize=16, fontweight='bold', y=0.995)
plt.tight_layout()
plt.savefig('training_convergence.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ“ Training convergence saved: training_convergence.png")

# ==================== 2. CONFUSION MATRIX + CLASSIFICATION METRICS ====================
print("\nðŸ“Š Plotting confusion matrix and metrics...")

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# 2.1 Confusion Matrix
ax = axes[0]
cm = confusion_matrix(test_labels, test_preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
            xticklabels=['Real', 'Fake'],
            yticklabels=['Real', 'Fake'],
            ax=ax, annot_kws={'size': 18})
ax.set_xlabel('Predicted Label', fontsize=13, fontweight='bold')
ax.set_ylabel('True Label', fontsize=13, fontweight='bold')
ax.set_title('Confusion Matrix', fontsize=15, fontweight='bold', pad=15)

# Add percentages
total = cm.sum()
for i in range(2):
    for j in range(2):
        percentage = cm[i, j] / cm[i].sum() * 100
        ax.text(j, i + 0.3, f'({percentage:.1f}%)',
                ha='center', va='center', fontsize=12, color='gray')

# 2.2 Classification Metrics
ax = axes[1]
precision, recall, f1, support = precision_recall_fscore_support(test_labels, test_preds)

x = np.arange(2)
width = 0.25

bars1 = ax.bar(x - width, precision, width, label='Precision',
               color='#FF9999', edgecolor='black', linewidth=1.5)
bars2 = ax.bar(x, recall, width, label='Recall',
               color='#66B3FF', edgecolor='black', linewidth=1.5)
bars3 = ax.bar(x + width, f1, width, label='F1-Score',
               color='#99FF99', edgecolor='black', linewidth=1.5)

ax.set_xlabel('Class', fontsize=13, fontweight='bold')
ax.set_ylabel('Score', fontsize=13, fontweight='bold')
ax.set_title('Performance Metrics by Class', fontsize=15, fontweight='bold', pad=15)
ax.set_xticks(x)
ax.set_xticklabels(['Real', 'Fake'], fontsize=12)
ax.legend(fontsize=11, loc='lower right')
ax.set_ylim([0.75, 1.05])
ax.grid(axis='y', alpha=0.3)

# Add value labels
for bars in [bars1, bars2, bars3]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.2%}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.savefig('confusion_matrix_metrics.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ“ Confusion matrix & metrics saved: confusion_matrix_metrics.png")

# ==================== 3. PLATFORM PERFORMANCE + GATE ANALYSIS ====================
print("\nðŸ“Š Plotting platform performance...")

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# 3.1 Platform Performance
ax = axes[0]
fb_mask = test_df['author_id'].str.contains('unknown')
tt_mask = ~fb_mask

fb_acc = test_df[fb_mask]['correct'].mean()
tt_acc = test_df[tt_mask]['correct'].mean()

sources = ['Facebook\n(unknown)', 'TikTok\n(valid author)']
accuracies = [fb_acc, tt_acc]
colors = ['#FF6B6B', '#4ECDC4']

bars = ax.bar(sources, accuracies, color=colors, alpha=0.8,
              edgecolor='black', linewidth=2, width=0.6)
ax.set_ylabel('Accuracy', fontsize=13, fontweight='bold')
ax.set_title('Performance by Platform', fontsize=15, fontweight='bold', pad=15)
ax.set_ylim([0.85, 1.0])
ax.grid(axis='y', alpha=0.3)

for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
            f'{acc:.2%}',
            ha='center', va='bottom', fontsize=14, fontweight='bold')

# Add sample counts
ax.text(0, 0.86, f'n={fb_mask.sum()}', ha='center', fontsize=10, color='gray')
ax.text(1, 0.86, f'n={tt_mask.sum()}', ha='center', fontsize=10, color='gray')

# 3.2 Gate Distribution
ax = axes[1]
unknown_gates = test_df[test_df['author_id'].str.contains('unknown')]['gate']
valid_gates = test_df[~test_df['author_id'].str.contains('unknown')]['gate']
unknown_gate = unknown_gates.mean()
valid_gate = valid_gates.mean()

ax.hist(unknown_gates, bins=30, alpha=0.7, label='Unknown (Facebook)',
        color='#FF6B6B', density=True, edgecolor='black')
ax.hist(valid_gates, bins=30, alpha=0.7, label='Valid (TikTok)',
        color='#4ECDC4', density=True, edgecolor='black')
ax.axvline(unknown_gate, color='red', linestyle='--', linewidth=2,
           label=f'Unknown: {unknown_gate:.3f}')
ax.axvline(valid_gate, color='blue', linestyle='--', linewidth=2,
           label=f'Valid: {valid_gate:.3f}')
ax.set_xlabel('Gate Value (Author Trust)', fontsize=12, fontweight='bold')
ax.set_ylabel('Density', fontsize=12, fontweight='bold')
ax.set_title('Gate Mechanism Learning', fontsize=15, fontweight='bold', pad=15)
ax.legend(fontsize=10, loc='upper right')
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('platform_gate_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ“ Platform & gate analysis saved: platform_gate_analysis.png")

# ==================== 4. PRINT TEXT CLASSIFICATION REPORT ====================
print("\n" + "="*70)
print("ðŸ“Š CLASSIFICATION REPORT")
print("="*70)
print("\n" + classification_report(test_labels, test_preds, target_names=['Real', 'Fake'], digits=4))

# ==================== 5. SUMMARY TABLE ====================
print("\n" + "="*70)
print("ðŸ“Š FINAL SUMMARY")
print("="*70)

summary_stats = pd.DataFrame({
    'Metric': [
        'Overall Accuracy',
        'Overall F1-Score',
        'Facebook Accuracy',
        'TikTok Accuracy',
        'Real Precision',
        'Real Recall',
        'Fake Precision',
        'Fake Recall',
        'Avg Gate (Unknown)',
        'Avg Gate (Valid)',
        'Train-Val Gap',
        'Val-Test Gap'
    ],
    'Value': [
        f'{test_acc:.2%}',
        f'{test_f1:.2%}',
        f'{fb_acc:.2%}',
        f'{tt_acc:.2%}',
        f'{precision[0]:.2%}',
        f'{recall[0]:.2%}',
        f'{precision[1]:.2%}',
        f'{recall[1]:.2%}',
        f'{unknown_gate:.4f}',
        f'{valid_gate:.4f}',
        f'{(final_train_acc - final_val_acc)*100:.2f}%',
        f'{(final_val_acc - test_acc)*100:.2f}%'
    ]
})

print(summary_stats.to_string(index=False))

print("\nâœ… Visualizations completed!")
print("   - training_convergence.png")
print("   - confusion_matrix_metrics.png")
print("   - platform_gate_analysis.png")

# ==================== AUTO DOWNLOAD (FIXED) ====================
print("\n" + "="*70)
print("ðŸ“¥ CREATING DOWNLOAD PACKAGE")
print("="*70)

def create_download_package(save_dir, output_name='phobert_model_package'):
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    zip_filename = f'{output_name}_{timestamp}.zip'

    print(f"\nðŸ“¦ Creating package: {zip_filename}")

    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(save_dir):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, os.path.dirname(save_dir))
                zipf.write(file_path, arcname)
                print(f"  âœ“ Added: {arcname}")

    zip_size = os.path.getsize(zip_filename) / (1024**2)
    print(f"\nâœ… Package created: {zip_filename} ({zip_size:.1f} MB)")

    return zip_filename, zip_size

zip_file, zip_size = create_download_package('./phobert_for_onnx', 'phobert_model_package')

# âœ… FIXED: Proper README without syntax error
readme_path = os.path.join(os.path.dirname(zip_file), 'README.txt')
with open(readme_path, 'w', encoding='utf-8') as f:
    f.write(f"""PhoBERT Fake News Detection Model Package
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Model Performance:
- Test Accuracy: {test_acc:.2%}
- Test F1-Score: {test_f1:.2%}
- Validation Accuracy: {best_val_acc:.2%}
- Training Epochs: {len(history['train_acc'])}

Package Contents:
1. best_model_weights.pt - Model weights (~135MB)
2. model_config.json - Architecture config
3. author_encoder.pkl - Author encoder
4. author_classes.json - Author list
5. label_mapping.json - Label mapping
6. tokenizer/ - PhoBERT tokenizer
7. training_history.json - Training metrics
8. sample_test.json - Test sample

Quick Start:
Load model with PyTorch and run inference.
See documentation for details.

Performance Summary:
- Facebook: {fb_acc:.2%} accuracy
- TikTok: {tt_acc:.2%} accuracy
- Gate (Unknown): {unknown_gate:.4f}
- Gate (Valid): {valid_gate:.4f}
""")

print(f"\nâœ“ README created: {readme_path}")

# Download in Colab
try:
    from google.colab import files
    print("\nðŸ“¥ Downloading...")
    files.download(zip_file)
    print("âœ… Download started!")
except ImportError:
    print(f"\nðŸ“ File saved: {os.path.abspath(zip_file)}")

print("\n" + "="*70)
print("ðŸŽ‰ ALL DONE!")
print("="*70)

"""###Convert to ONNX"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install onnx onnxruntime

import zipfile
import os

# Extract ZIP
zip_path = 'phobert_model_package_20251118_085406.zip'
extract_dir = './phobert_onnx_conversion'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("âœ“ Extracted files:")
for root, dirs, files in os.walk(extract_dir):
    for file in files:
        print(f"  - {os.path.join(root, file)}")

# ==================== 1. DEFINE MODEL ARCHITECTURE ====================
class PhoBERTFakeNewsDetector(nn.Module):
    def __init__(self, num_authors, author_embed_dim=64, dropout_rate=0.3):
        super(PhoBERTFakeNewsDetector, self).__init__()

        self.phobert = RobertaModel.from_pretrained("vinai/phobert-base-v2")
        self.author_embedding = nn.Embedding(num_authors, author_embed_dim)

        self.author_gate = nn.Sequential(
            nn.Linear(author_embed_dim, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

        self.text_branch = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 64),
            nn.ReLU()
        )

        self.combined_branch = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(768 + author_embed_dim, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 64),
            nn.ReLU()
        )

        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(64, 2)
        )

    def forward(self, input_ids, attention_mask, author_ids, has_valid_author=None):
        # Get PhoBERT features
        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)
        text_features = outputs.last_hidden_state[:, 0, :]

        # Get author embedding
        author_embed = self.author_embedding(author_ids)

        # Calculate gate
        gate = self.author_gate(author_embed)

        # Apply mask
        if has_valid_author is not None:
            valid_mask = has_valid_author.unsqueeze(1).float()
            gate = gate * valid_mask + 0.05 * (1 - valid_mask)

        # Branches
        text_out = self.text_branch(text_features)
        combined_in = torch.cat([text_features, author_embed], dim=1)
        combined_out = self.combined_branch(combined_in)

        # Fusion
        features = (1 - gate) * text_out + gate * combined_out

        # Prediction
        logits = self.classifier(features)

        return logits, gate

# ==================== 2. LOAD CONFIG ====================
print("\nðŸ“‚ Loading configuration...")
base_dir = './phobert_onnx_conversion/phobert_for_onnx'

with open(f'{base_dir}/model_config.json', 'r') as f:
    config = json.load(f)

print(f"âœ“ Config loaded:")
print(f"  - Num authors: {config['num_authors']}")
print(f"  - Val accuracy: {config['val_acc']:.4f}")
print(f"  - Val F1: {config['val_f1']:.4f}")

# ==================== 3. LOAD MODEL ====================
print("\nðŸ—ï¸ Reconstructing model...")

model = PhoBERTFakeNewsDetector(
    num_authors=config['num_authors'],
    author_embed_dim=64,
    dropout_rate=0.3
)

# Load weights
checkpoint = torch.load(f'{base_dir}/best_model_weights.pt', map_location='cpu')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

print("âœ“ Model loaded successfully")

# ==================== 4. PREPARE DUMMY INPUT ====================
print("\nðŸ“ Preparing dummy input...")

tokenizer = AutoTokenizer.from_pretrained(f'{base_dir}/tokenizer')

# Sample Vietnamese text
dummy_text = "Bá»™ trÆ°á»Ÿng Bruno Bruins pháº£i tá»« chá»©c sau khi ngáº¥t xá»‰u khi Ä‘ang tháº£o luáº­n vá» Covid-19 táº¡i nghá»‹ viá»‡n HÃ  Lan."

encoding = tokenizer(
    dummy_text,
    max_length=256,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)

dummy_input = (
    encoding['input_ids'],           # [1, 256]
    encoding['attention_mask'],      # [1, 256]
    torch.tensor([0], dtype=torch.long),     # author_id
    torch.tensor([False], dtype=torch.bool)  # has_valid_author
)

print(f"âœ“ Dummy input prepared:")
print(f"  - Text: {dummy_text[:60]}...")
print(f"  - Input shape: {dummy_input[0].shape}")

# ==================== 5. TEST PYTORCH MODEL ====================
print("\nðŸ§ª Testing PyTorch model...")

with torch.no_grad():
    pytorch_logits, pytorch_gate = model(*dummy_input)
    pytorch_probs = torch.softmax(pytorch_logits, dim=1)
    pytorch_pred = torch.argmax(pytorch_probs, dim=1).item()

print(f"âœ“ PyTorch prediction:")
print(f"  - Label: {'Fake' if pytorch_pred == 1 else 'Real'}")
print(f"  - Confidence: {pytorch_probs[0][pytorch_pred].item():.4f}")
print(f"  - Probabilities: Real={pytorch_probs[0][0].item():.4f}, Fake={pytorch_probs[0][1].item():.4f}")
print(f"  - Gate value: {pytorch_gate[0][0].item():.4f}")

# ==================== 6. EXPORT TO ONNX ====================
print("\nðŸ”„ Exporting to ONNX...")

onnx_path = f'{base_dir}/phobert_fake_news.onnx'

# Set model to eval mode and disable dropout
model.eval()
for module in model.modules():
    if isinstance(module, nn.Dropout):
        module.p = 0.0  # Disable dropout for inference

torch.onnx.export(
    model,
    dummy_input,
    onnx_path,
    input_names=['input_ids', 'attention_mask', 'author_ids', 'has_valid_author'],
    output_names=['logits', 'gate'],
    dynamic_axes={
        'input_ids': {0: 'batch_size', 1: 'sequence'},
        'attention_mask': {0: 'batch_size', 1: 'sequence'},
        'author_ids': {0: 'batch_size'},
        'has_valid_author': {0: 'batch_size'},
        'logits': {0: 'batch_size'},
        'gate': {0: 'batch_size'}
    },
    opset_version=14,
    do_constant_folding=True,
    verbose=False
)

print(f"âœ“ Model exported to: {onnx_path}")

# ==================== 7. VERIFY ONNX MODEL ====================
print("\nâœ… Verifying ONNX model...")

import onnx
import onnxruntime as ort

# Check model
onnx_model = onnx.load(onnx_path)
onnx.checker.check_model(onnx_model)
print("âœ“ ONNX model structure is valid")

# Test inference
ort_session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])

ort_inputs = {
    'input_ids': encoding['input_ids'].numpy(),
    'attention_mask': encoding['attention_mask'].numpy(),
    'author_ids': np.array([0], dtype=np.int64),
    'has_valid_author': np.array([False], dtype=np.bool_)
}

ort_outputs = ort_session.run(None, ort_inputs)
onnx_logits = ort_outputs[0][0]
onnx_gate = ort_outputs[1][0][0]

# Softmax
onnx_exp = np.exp(onnx_logits - np.max(onnx_logits))
onnx_probs = onnx_exp / onnx_exp.sum()
onnx_pred = np.argmax(onnx_probs)

print(f"âœ“ ONNX prediction:")
print(f"  - Label: {'Fake' if onnx_pred == 1 else 'Real'}")
print(f"  - Confidence: {onnx_probs[onnx_pred]:.4f}")
print(f"  - Probabilities: Real={onnx_probs[0]:.4f}, Fake={onnx_probs[1]:.4f}")
print(f"  - Gate value: {onnx_gate:.4f}")

# ==================== 8. COMPARE PYTORCH VS ONNX ====================
print("\nðŸ“Š Comparison PyTorch vs ONNX:")

logits_diff = np.abs(pytorch_logits.numpy() - onnx_logits).max()
gate_diff = np.abs(pytorch_gate.numpy()[0][0] - onnx_gate)

print(f"  - Max logits difference: {logits_diff:.8f}")
print(f"  - Gate difference: {gate_diff:.8f}")

if logits_diff < 1e-4 and gate_diff < 1e-4:
    print("\nâœ… SUCCESS! ONNX model matches PyTorch model perfectly!")
else:
    print(f"\nâš ï¸ Warning: Difference detected (threshold: 1e-4)")

# ==================== 9. SAVE METADATA ====================
print("\nðŸ’¾ Saving ONNX metadata...")

onnx_metadata = {
    'model_path': 'phobert_fake_news.onnx',
    'input_names': ['input_ids', 'attention_mask', 'author_ids', 'has_valid_author'],
    'output_names': ['logits', 'gate'],
    'input_shapes': {
        'input_ids': [1, 256],
        'attention_mask': [1, 256],
        'author_ids': [1],
        'has_valid_author': [1]
    },
    'output_shapes': {
        'logits': [1, 2],
        'gate': [1, 1]
    },
    'max_logits_diff': float(logits_diff),
    'max_gate_diff': float(gate_diff),
    'opset_version': 14,
    'test_accuracy': config['val_acc'],
    'test_f1': config['val_f1']
}

with open(f'{base_dir}/onnx_metadata.json', 'w') as f:
    json.dump(onnx_metadata, f, indent=2)

print(f"âœ“ Metadata saved: {base_dir}/onnx_metadata.json")

# ==================== 10. FILE SIZE INFO ====================
print("\nðŸ“Š File sizes:")

pytorch_size = os.path.getsize(f'{base_dir}/best_model_weights.pt') / (1024**2)
onnx_size = os.path.getsize(onnx_path) / (1024**2)

print(f"  - PyTorch model: {pytorch_size:.1f} MB")
print(f"  - ONNX model: {onnx_size:.1f} MB")
print(f"  - Size ratio: {onnx_size/pytorch_size:.2f}x")

# ==================== SUMMARY ====================
print("\n" + "="*70)
print("âœ… CONVERSION COMPLETED SUCCESSFULLY!")
print("="*70)

print(f"""
ðŸ“ Output files:
  âœ“ ONNX model: {onnx_path} ({onnx_size:.1f} MB)
  âœ“ Metadata: {base_dir}/onnx_metadata.json

ðŸ“Š Verification:
  âœ“ Model structure: Valid
  âœ“ Inference test: Passed
  âœ“ PyTorch vs ONNX: Match (diff < 1e-4)

ðŸŽ¯ Test Results:
  - Accuracy: {config['val_acc']:.2%}
  - F1-Score: {config['val_f1']:.2%}
  - Prediction: {('Fake' if onnx_pred == 1 else 'Real')}
  - Confidence: {onnx_probs[onnx_pred]:.2%}

ðŸš€ Ready for deployment in browser extension!

Next steps:
  1. Copy {onnx_path} to your extension folder
  2. Use onnxruntime-web to run inference
  3. Test in browser
  4. Deploy!
""")