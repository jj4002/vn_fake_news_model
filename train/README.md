# Model Training & Experiments

Th∆∞ m·ª•c ch·ª©a c√°c script v√† notebook ƒë·ªÉ train v√† experiment v·ªõi nhi·ªÅu ki·∫øn tr√∫c model kh√°c nhau cho fake news detection tr√™n TikTok.

## üìã T·ªïng quan

D·ª± √°n n√†y bao g·ªìm **4 experiments ch√≠nh** v·ªõi c√°c approaches kh√°c nhau:

1. **Baseline PhoBERT** (`train-baseline-phobert.py`) - Sequence Classification ƒë∆°n gi·∫£n
2. **PhoBERT + Author Embedding** (`train-author-embedding.py`) - Multi-modal v·ªõi author information
3. **Prompt-based MLM** (`train-MLM_Prompt.py`) - Masked Language Modeling v·ªõi prompts
4. **HAN + RAG** (`train-rag-han.ipynb`) - Hierarchical Attention Network v·ªõi RAG (Production)

## üìÅ Files

```
train/
‚îú‚îÄ‚îÄ train-baseline-phobert.py    # Experiment 1: Baseline PhoBERT
‚îú‚îÄ‚îÄ train-author-embedding.py    # Experiment 2: PhoBERT + Author Embedding
‚îú‚îÄ‚îÄ train-MLM_Prompt.py          # Experiment 3: Prompt-based MLM
‚îî‚îÄ‚îÄ train-rag-han.ipynb          # Experiment 4: HAN + RAG (Production)
```

## üî¨ Experiments Overview

### Experiment 1: Baseline PhoBERT (`train-baseline-phobert.py`)

**M·ª•c ƒë√≠ch:** Baseline ƒë∆°n gi·∫£n v·ªõi PhoBERT sequence classification

**Ki·∫øn tr√∫c:**
- **Model**: `RobertaForSequenceClassification`
- **Input**: Text only (title + content)
- **Output**: Binary classification (REAL/FAKE)

**Hyperparameters:**
- Learning rate: 2e-5
- Batch size: 16
- Epochs: 5
- Max length: 256 tokens
- Optimizer: AdamW
- Loss: CrossEntropyLoss

**K·∫øt qu·∫£:** Baseline performance ƒë·ªÉ so s√°nh v·ªõi c√°c models kh√°c

---

### Experiment 2: PhoBERT + Author Embedding (`train-author-embedding.py`)

**M·ª•c ƒë√≠ch:** T·∫≠n d·ª•ng th√¥ng tin author ƒë·ªÉ c·∫£i thi·ªán accuracy

**Ki·∫øn tr√∫c:**
- **Backbone**: PhoBERT-base-v2
- **Author Embedding**: Embedding layer cho t·ª´ng author
- **Adaptive Gating**: T·ª± ƒë·ªông h·ªçc khi n√†o tin author, khi n√†o ch·ªâ d√πng text
- **Dual Branch**: 
  - Text-only branch (cho unknown authors)
  - Combined branch (text + author embedding)

**Features:**
- Author encoding v·ªõi LabelEncoder
- Gating mechanism ƒë·ªÉ ƒëi·ªÅu ch·ªânh importance c·ªßa author
- Weighted Focal Loss v·ªõi label smoothing
- Mixed precision training (FP16)

**Hyperparameters:**
- Learning rate: 2e-5 (different rates cho t·ª´ng component)
- Batch size: 16
- Epochs: 8
- Author embedding dim: 64
- Dropout: 0.3
- Focal loss: alpha=0.7, gamma=2

**K·∫øt qu·∫£:** C·∫£i thi·ªán ƒë√°ng k·ªÉ khi c√≥ author information

---

### Experiment 3: Prompt-based MLM (`train-MLM_Prompt.py`)

**M·ª•c ƒë√≠ch:** Fine-tune PhoBERT v·ªõi Masked Language Modeling v√† prompt engineering

**Ki·∫øn tr√∫c:**
- **Model**: `AutoModelForMaskedLM` (PhoBERT MLM)
- **Prompt Format**: `"B√†i vi·∫øt n√†y l√† <mask> . Ti√™u_ƒë·ªÅ : {title} . N·ªôi_dung : {content}"`
- **Verbalizer**: 
  - Label 0 (REAL) ‚Üí token "th·∫≠t"
  - Label 1 (FAKE) ‚Üí token "gi·∫£"
- **Training**: Predict token t·∫°i v·ªã tr√≠ `<mask>`

**Features:**
- Vietnamese text normalizer (kh√¥ng c·∫ßn vinorm)
- Teencode handling
- Word segmentation v·ªõi underthesea
- Class-weighted loss
- Gradient accumulation

**Hyperparameters:**
- Learning rate: 2e-5
- Batch size: 16
- Gradient accumulation: 2 steps
- Epochs: 4
- Max length: 256 tokens
- Warmup: 10% of total steps

**K·∫øt qu·∫£:** T·∫≠n d·ª•ng pre-trained knowledge t·ªët h∆°n v·ªõi prompt

---

### Experiment 4: HAN + RAG (`train-rag-han.ipynb`) ‚≠ê **PRODUCTION**

**M·ª•c ƒë√≠ch:** Hierarchical Attention Network v·ªõi RAG verification (model ƒë∆∞·ª£c s·ª≠ d·ª•ng trong production)

**Ki·∫øn tr√∫c:**
- **HAN Model**: 
  - Chunk content th√†nh segments
  - RAG-based chunk selection (top-k chunks d·ª±a tr√™n title similarity)
  - Hierarchical attention (chunk-level ‚Üí document-level)
- **RAG Integration**:
  - Vector search trong news corpus
  - Similarity threshold: 0.75
  - Confidence adjustment d·ª±a tr√™n matching articles

**Features:**
- Text normalization gi·ªëng training
- Semantic chunk retriever v·ªõi SentenceTransformer
- ONNX export cho production
- Cache mechanism

**Hyperparameters:**
- Learning rate: 2e-5
- Batch size: 16
- Epochs: 5-10
- Max length: 256 tokens
- Chunk size: 400 chars
- Top-k chunks: 5

**K·∫øt qu·∫£:** Best performance v·ªõi RAG verification, ƒë∆∞·ª£c deploy trong production

---

## üìä So s√°nh Experiments

| Experiment | Model | Input Features | Complexity | Performance | Use Case |
|------------|-------|----------------|------------|-------------|----------|
| 1. Baseline | PhoBERT SC | Text only | Low | Baseline | Quick test |
| 2. Author Embed | PhoBERT + Author | Text + Author | Medium | Good | When author info available |
| 3. Prompt MLM | PhoBERT MLM | Text + Prompt | Medium | Good | Leverage pre-trained knowledge |
| 4. HAN + RAG | HAN + RAG | Text + Chunks | High | **Best** | **Production** |

## üöÄ Training Pipeline (Chung cho t·∫•t c·∫£ experiments)

### 1. Data Preparation

**Input:**
- Dataset t·ª´ `crawl/` folder
- Format: CSV v·ªõi columns `title`, `content` (ho·∫∑c `text`), `label`
- Optional: `author_id` (cho Experiment 2)

**Preprocessing:**
- Text normalization (Vietnamese)
- Word segmentation v·ªõi underthesea
- Chunking content th√†nh segments (cho HAN)
- Train/val/test split (stratified)

### 2. Training Process

**Common steps:**
1. Load v√† preprocess data
2. Initialize model v√† tokenizer
3. Create DataLoaders
4. Setup optimizer v√† scheduler
5. Train v·ªõi validation
6. Evaluate tr√™n test set
7. Export model (ONNX cho production)

### 3. Evaluation Metrics

- **Accuracy**: Overall correctness
- **Precision/Recall**: Per-class metrics
- **F1-score**: Weighted F1
- **Confusion Matrix**: Visual representation
- **ROC-AUC**: (Optional) Area under curve

## üìù Usage

### Setup Environment

```bash
# Core dependencies
pip install torch transformers sentence-transformers
pip install underthesea  # Vietnamese NLP
pip install onnx onnxruntime
pip install pandas numpy scikit-learn

# Additional for specific experiments
pip install ydata_profiling  # For data profiling (train-baseline-phobert.py)
pip install optimum[onnxruntime]  # For ONNX export (train-MLM_Prompt.py)
```

### Run Experiments

#### Experiment 1: Baseline PhoBERT

```bash
python train-baseline-phobert.py
```

**Input files:**
- `combined_train.csv` - Combined training data
- `val_clean.csv` - Validation set
- `test_clean.csv` - Test set

**Output:**
- `best_phobert_fake_news.pt` - Best model weights
- `phobert_fake_news_model/` - Saved model directory

#### Experiment 2: PhoBERT + Author Embedding

```bash
python train-author-embedding.py
```

**Input files:**
- `final_train_stratified.csv` - Training v·ªõi author_id
- `final_val_stratified.csv` - Validation v·ªõi author_id
- `final_test_stratified.csv` - Test v·ªõi author_id

**Output:**
- `phobert_for_onnx/best_model_weights.pt` - Model weights
- `phobert_for_onnx/model_config.json` - Config
- `phobert_for_onnx/author_classes.json` - Author mappings
- `phobert_fake_news.onnx` - ONNX model

#### Experiment 3: Prompt-based MLM

```bash
python train-MLM_Prompt.py
```

**Input:**
- Merged dataset v·ªõi `title`, `text`, `label` columns

**Output:**
- Trained MLM model
- Evaluation metrics

#### Experiment 4: HAN + RAG (Production)

1. M·ªü notebook: `train-rag-han.ipynb`
2. C·∫•u h√¨nh paths:
   - Dataset path
   - Model save path
   - Output path
3. Ch·∫°y cells theo th·ª© t·ª±

**Export to ONNX:**

```python
# Export HAN model to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "han_rag_model.onnx",
    input_names=['chunk_input_ids', 'chunk_attention_masks'],
    output_names=['logits'],
    dynamic_axes={
        'chunk_input_ids': {0: 'batch_size'},
        'chunk_attention_masks': {0: 'batch_size'}
    }
)
```

## üîß Configuration

### Data Paths (T√πy theo experiment)

**Experiment 1:**
```python
TRAIN_CSV = "combined_train.csv"
VAL_CSV = "val_clean.csv"
TEST_CSV = "test_clean.csv"
```

**Experiment 2:**
```python
TRAIN_CSV = "final_train_stratified.csv"
VAL_CSV = "final_val_stratified.csv"
TEST_CSV = "final_test_stratified.csv"
```

**Experiment 4 (HAN):**
```python
TRAIN_CSV = "../crawl/fake_all.csv"
VAL_CSV = "../crawl/val_data.csv"
TEST_CSV = "../crawl/test_data.csv"
```

### Model Config (Chung)

```python
MODEL_NAME = "vinai/phobert-base-v2"
MAX_LENGTH = 256
NUM_LABELS = 2
```

**HAN-specific:**
```python
CHUNK_SIZE = 400
TOP_K_CHUNKS = 5
RETRIEVER_MODEL = "keepitreal/vietnamese-sbert"
```

**Author Embedding (Exp 2):**
```python
AUTHOR_EMBED_DIM = 64
DROPOUT_RATE = 0.3
```

### Training Config (Chung)

```python
BATCH_SIZE = 16
LEARNING_RATE = 2e-5
NUM_EPOCHS = 5-8  # T√πy experiment
WARMUP_RATIO = 0.1
WEIGHT_DECAY = 0.01-0.02
```

**Experiment-specific:**
- **Exp 2**: Different learning rates cho t·ª´ng component
- **Exp 3**: Gradient accumulation = 2
- **Exp 4**: Chunk-based processing

## üìä Dataset Requirements

### Format

CSV v·ªõi columns:
- `title`: Video caption/title
- `content`: OCR + STT text (ho·∫∑c ch·ªâ caption n·∫øu kh√¥ng c√≥)
- `label`: `FAKE` ho·∫∑c `REAL`

### Size Recommendations

- **Minimum**: 1000 samples m·ªói class
- **Recommended**: 5000+ samples m·ªói class
- **Ideal**: 10000+ samples m·ªói class

### Data Balance

- C√¢n b·∫±ng gi·ªØa FAKE v√† REAL
- N·∫øu kh√¥ng c√¢n b·∫±ng, s·ª≠ d·ª•ng class weights

## üß™ Evaluation

### Metrics

```python
# Calculate metrics
accuracy = (tp + tn) / (tp + tn + fp + fn)
precision = tp / (tp + fp)
recall = tp / (tp + fn)
f1 = 2 * (precision * recall) / (precision + recall)
```

### Validation

- Validation tr√™n held-out set
- Early stopping n·∫øu validation loss kh√¥ng gi·∫£m
- Save best model d·ª±a tr√™n F1-score

## üêõ Troubleshooting

### Out of Memory

**V·∫•n ƒë·ªÅ:** CUDA out of memory
- **Gi·∫£i ph√°p:**
  - Gi·∫£m batch size
  - Gi·∫£m max_length
  - S·ª≠ d·ª•ng gradient accumulation

### Training kh√¥ng converge

**V·∫•n ƒë·ªÅ:** Loss kh√¥ng gi·∫£m
- **Gi·∫£i ph√°p:**
  - Check learning rate
  - Check data quality
  - Try different optimizers
  - Add warmup steps

### Overfitting

**V·∫•n ƒë·ªÅ:** Train accuracy cao nh∆∞ng val th·∫•p
- **Gi·∫£i ph√°p:**
  - Add dropout
  - Increase weight decay
  - Add more data
  - Early stopping

## üìà Best Practices

1. **Data Quality**: Clean v√† validate data k·ªπ
2. **Cross-validation**: S·ª≠ d·ª•ng k-fold n·∫øu dataset nh·ªè
3. **Hyperparameter tuning**: Grid search ho·∫∑c random search
4. **Model checkpointing**: Save model m·ªói epoch
5. **Logging**: Log metrics v√† losses
6. **Reproducibility**: Set random seeds

## üîí Model Security

- **Model validation**: Test model tr√™n edge cases
- **Bias checking**: Check bias tr√™n different groups
- **Adversarial testing**: Test v·ªõi adversarial examples

## üîÆ Future Improvements

- [ ] Multi-task learning
- [ ] Transfer learning t·ª´ models kh√°c
- [ ] Ensemble methods
- [ ] Hyperparameter optimization v·ªõi Optuna
- [ ] Model distillation
- [ ] Quantization cho mobile deployment

## üìö References

### Papers & Models

- **HAN**: [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)
- **PhoBERT**: [PhoBERT: Pre-trained language models for Vietnamese](https://arxiv.org/abs/2003.00744)
- **Prompt Learning**: [GPT-3 Paper](https://arxiv.org/abs/2005.14165) (inspiration)
- **Focal Loss**: [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)

### Technical Docs

- **ONNX Export**: [PyTorch to ONNX](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)
- **Transformers**: [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- **Sentence Transformers**: [Sentence-BERT](https://www.sbert.net/)

### Datasets

- **[Vietnamese Fake News Detection](https://github.com/hiepnguyenduc2005/Vietnamese-Fake-News-Detection)**: Dataset t·ª´ ReINTEL v·ªõi g·∫ßn 10,000 examples ƒë∆∞·ª£c g√°n nh√£n. Dataset n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng ch√≠nh cho training baseline models v√† c√°c experiments.
- **[VFND Vietnamese Fake News Datasets](https://github.com/WhySchools/VFND-vietnamese-fake-news-datasets)**: T·∫≠p h·ª£p c√°c b√†i b√°o ti·∫øng Vi·ªát v√† Facebook posts ƒë∆∞·ª£c ph√¢n lo·∫°i (228-254 b√†i), bao g·ªìm c·∫£ Article Contents v√† Social Contents. Dataset n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ b·ªï sung v√† ƒëa d·∫°ng h√≥a training data.

## üìÑ License

MIT License

