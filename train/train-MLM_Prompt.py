# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kaFR52rQ1nGbkpyxO5nvraN8KGTclkSz
"""

# Commented out IPython magic to ensure Python compatibility.
# Block 1: Installation
# %pip install transformers==4.35.0
# %pip install torch torchvision torchaudio
# %pip install underthesea
# %pip install onnx onnxruntime
# %pip install optimum[onnxruntime]
# %pip install datasets
# %pip install accelerate -U

# Block 2: Vietnamese Text Normalizer (NO VINORM REQUIRED)
import re
import unicodedata
from typing import Dict, Optional

try:
    from underthesea import word_tokenize
    UNDERTHESEA_AVAILABLE = True
except:
    UNDERTHESEA_AVAILABLE = False
    print("Warning: underthesea not available")


class VietnameseTextNormalizer:
    """Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát - KH√îNG C·∫¶N vinorm"""

    def __init__(self):
        # Teencode dictionary
        self.teencode_dict: Dict[str, str] = {
            'mk': 'm√¨nh', 'mik': 'm√¨nh', 'bn': 'b·∫°n', 'b': 'b·∫°n',
            'tui': 't√¥i', 't': 't√¥i', 'toy': 't√¥i',
            'ko': 'kh√¥ng', 'k': 'kh√¥ng', 'hok': 'kh√¥ng', 'kg': 'kh√¥ng',
            'dc': 'ƒë∆∞·ª£c', 'ƒëc': 'ƒë∆∞·ª£c', 'dk': 'ƒë∆∞·ª£c',
            'j': 'g√¨', 'z': 'g√¨', 'gj': 'g√¨',
            'vs': 'v·ªõi', 'v': 'v·ªõi', 'w': 'v·ªõi',
            'ntn': 'nh∆∞ th·∫ø n√†o', 'nt': 'nh∆∞ th·∫ø',
            'tks': 'c·∫£m ∆°n', 'ty': 'c·∫£m ∆°n', 'thanks': 'c·∫£m ∆°n',
            'sry': 'xin l·ªói', 'sr': 'xin l·ªói',
            'nh√¨u': 'nhi·ªÅu', 'nhiu': 'nhi·ªÅu',
            'ƒëag': 'ƒëang', 'dg': 'ƒëang',
            'cx': 'c≈©ng', 'cug': 'c≈©ng',
            'lm': 'l√†m', 'lam': 'l√†m',
            'th': 'th√¨', 'thi': 'th√¨',
            'fai': 'ph·∫£i', 'p·∫£i': 'ph·∫£i',
            'pic': '·∫£nh', 'img': '·∫£nh',
            'ad': 'admin',
            'ib': 'inbox', 'cmt': 'comment',
            'lol': 'c∆∞·ªùi', 'haha': 'c∆∞·ªùi',
            'oke': 'ok', 'okie': 'ok',
            'wa': 'qu√°', 'w√°': 'qu√°',
            'r': 'r·ªìi', 'oy': 'r·ªìi',
            'nha': 'nh√©', 'nhe': 'nh√©',
            'biet': 'bi·∫øt', 'bik': 'bi·∫øt',
            'muon': 'mu·ªën', 'mun': 'mu·ªën',
        }

        self.use_word_segment = UNDERTHESEA_AVAILABLE

    def normalize_unicode(self, text: str) -> str:
        """Chu·∫©n h√≥a Unicode v·ªÅ d·∫°ng NFC"""
        return unicodedata.normalize('NFC', text)

    def clean_special_chars(self, text: str) -> str:
        """L√†m s·∫°ch k√Ω t·ª± ƒë·∫∑c bi·ªát"""
        # Remove emojis
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"
            u"\U0001F300-\U0001F5FF"
            u"\U0001F680-\U0001F6FF"
            u"\U0001F1E0-\U0001F1FF"
            "]+", flags=re.UNICODE)
        text = emoji_pattern.sub(' ', text)

        # Remove URLs
        text = re.sub(r'http[s]?://\S+', ' ', text)

        # Keep Vietnamese chars, alphanumeric, basic punctuation, and special tokens
        text = re.sub(r'[^\w\s.,!?√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒëƒê<>]', ' ', text)

        return text

    def handle_teencode(self, text: str) -> str:
        """Thay th·∫ø teencode"""
        words = text.split()
        normalized_words = []

        for word in words:
            word_lower = word.lower()
            word_clean = re.sub(r'[.,!?]', '', word_lower)

            if word_clean in self.teencode_dict:
                normalized_words.append(self.teencode_dict[word_clean])
            else:
                normalized_words.append(word)

        return ' '.join(normalized_words)

    def standardize_numbers_dates(self, text: str) -> str:
        """
        Chu·∫©n h√≥a s·ªë - ENHANCED REGEX (thay vinorm)
        """
        # Number + k/tr/t·ª∑
        text = re.sub(r'(\d+)\s*k\b', r'\1 ngh√¨n', text, flags=re.IGNORECASE)
        text = re.sub(r'(\d+)\s*tr\b', r'\1 tri·ªáu', text, flags=re.IGNORECASE)
        text = re.sub(r'(\d+)\s*t·ª∑\b', r'\1 t·ª∑', text, flags=re.IGNORECASE)
        text = re.sub(r'(\d+)\s*t·ªâ\b', r'\1 t·ª∑', text, flags=re.IGNORECASE)

        # Dates: 12/5/2024 ‚Üí m∆∞·ªùi hai th√°ng nƒÉm nƒÉm hai ngh√¨n hai m∆∞∆°i b·ªën
        # (Simple version - keep as numbers for PhoBERT)
        # PhoBERT c√≥ th·ªÉ hi·ªÉu s·ªë t·ªët, kh√¥ng c·∫ßn convert sang ch·ªØ

        # Remove extra spaces
        text = re.sub(r'\s+', ' ', text)

        return text.strip()

    def word_segment(self, text: str) -> str:
        """T√°ch t·ª´ v·ªõi b·∫£o v·ªá special tokens"""
        if not self.use_word_segment:
            return text

        # Protect special tokens
        special_tokens = ['<mask>', '<s>', '</s>', '<pad>', '<unk>']
        placeholders = {}

        for i, token in enumerate(special_tokens):
            placeholder = f"___SPECIAL_TOKEN_{i}___"
            if token in text:
                placeholders[placeholder] = token
                text = text.replace(token, placeholder)

        try:
            text = word_tokenize(text, format="text")

            # Restore special tokens
            for placeholder, token in placeholders.items():
                text = text.replace(placeholder, token)
        except Exception as e:
            print(f"Word segmentation error: {e}")

        return text

    def normalize(self, text: Optional[str], preserve_mask: bool = False) -> str:
        """Pipeline chu·∫©n h√≥a ƒë·∫ßy ƒë·ªß"""
        if not text or not isinstance(text, str):
            return ""

        # Step 1: Unicode normalization
        text = self.normalize_unicode(text)

        # Step 2: Basic cleaning
        text = text.strip()
        text = self.clean_special_chars(text)
        text = re.sub(r'\s+', ' ', text)

        # Step 3: Teencode handling
        text = self.handle_teencode(text)

        # Step 4: Number/date standardization (NO vinorm needed)
        text = self.standardize_numbers_dates(text)

        # Step 5: Word segmentation (if not preserving mask)
        if not preserve_mask:
            text = self.word_segment(text)

        # Final cleanup
        text = re.sub(r'\s+', ' ', text).strip()

        return text


# Test normalizer
def test_normalizer():
    """Test without vinorm"""
    normalizer = VietnameseTextNormalizer()

    test_cases = [
        ("mk ko bik j h·∫øt √°", "Teencode"),
        ("Gi√° 500k th√¥i nha", "Numbers"),
        ("Tin n√†y fake 100% lun", "Mixed"),
        ("W√° hay, dc 10tr view", "Social media style"),
    ]

    print("="*60)
    print("NORMALIZER TEST (NO VINORM REQUIRED)")
    print("="*60)

    for text, desc in test_cases:
        normalized = normalizer.normalize(text)
        print(f"\n[{desc}]")
        print(f"Input : {text}")
        print(f"Output: {normalized}")

    print("\n" + "="*60)
    print("‚úÖ Normalizer ready! (vinorm not required)")
    print("="*60)

# Run test
test_normalizer()

# Block 3: Custom Dataset (FINAL FIX - Reverse Prompt Order)
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer
import pandas as pd
from typing import List, Dict


class VietnameseFakeNewsDataset(Dataset):
    """Dataset class cho Prompt-based Fake News Detection"""

    def __init__(
        self,
        data: pd.DataFrame,
        tokenizer: AutoTokenizer,
        normalizer: VietnameseTextNormalizer,
        max_length: int = 256
    ):
        self.data = data
        self.tokenizer = tokenizer
        self.normalizer = normalizer
        self.max_length = max_length

        # Verify mask token
        print(f"Tokenizer mask token: '{self.tokenizer.mask_token}'")
        print(f"Tokenizer mask token ID: {self.tokenizer.mask_token_id}")

        # Verbalizer
        self.label_to_token = {
            0: "th·∫≠t",
            1: "gi·∫£"
        }

        # Get token IDs
        self.label_token_ids = {
            label: self.tokenizer.convert_tokens_to_ids(token)
            for label, token in self.label_to_token.items()
        }

        for label, token in self.label_to_token.items():
            print(f"Label {label} ('{token}'): token_id = {self.label_token_ids[label]}")

    def create_prompt_text(self, row: pd.Series) -> str:
        """
        CRITICAL FIX: ƒê·∫∑t <mask> ·ªû ƒê·∫¶U ƒë·ªÉ tr√°nh b·ªã truncate
        """
        # Normalize
        title_raw = row.get('title', '')
        text_raw = row.get('text', '')

        title_clean = self.normalizer.normalize(title_raw, preserve_mask=True)
        text_clean = self.normalizer.normalize(text_raw, preserve_mask=True)

        # Word segmentation
        title_seg = self.normalizer.word_segment(title_clean) if title_clean else ""
        text_seg = self.normalizer.word_segment(text_clean) if text_clean else ""

        # Get mask token
        mask_token = self.tokenizer.mask_token

        # NEW STRATEGY: ƒê·∫∑t <mask> ·ªü ƒë·∫ßu ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªã truncate
        # Format: "B√†i vi·∫øt n√†y l√† <mask> . Ti√™u_ƒë·ªÅ : ... . N·ªôi_dung : ..."
        prompt = f"B√†i vi·∫øt n√†y l√† {mask_token} . Ti√™u_ƒë·ªÅ : {title_seg} . N·ªôi_dung : {text_seg}"

        return prompt

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]

        # Create prompt
        prompt_text = self.create_prompt_text(row)

        # Tokenize
        encoding = self.tokenizer(
            prompt_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # Find <mask> position
        mask_token_id = self.tokenizer.mask_token_id
        mask_positions = (encoding['input_ids'] == mask_token_id).nonzero(as_tuple=True)[1]

        if len(mask_positions) == 0:
            # Debug
            decoded = self.tokenizer.decode(encoding['input_ids'][0], skip_special_tokens=False)
            print(f"\n‚ùå DEBUG INFO:")
            print(f"Prompt length: {len(prompt_text)} chars")
            print(f"Prompt start: {prompt_text[:100]}")
            print(f"Decoded start: {decoded[:100]}")
            print(f"Mask token in prompt: {mask_token in prompt_text}")

            raise ValueError(
                f"‚ùå Mask token not found! It was likely truncated.\n"
                f"Solution: Reduce text length or increase max_length"
            )

        mask_pos = mask_positions[0].item()

        # Get label
        label = int(row['label'])
        label_token_id = self.label_token_ids[label]

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'mask_position': mask_pos,
            'labels': label_token_id,
            'label_class': label
        }

# Block 4: Training with Weighted Loss (COMPLETE VERSION - Train/Val/Test Split)
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoModelForMaskedLM, AutoTokenizer, get_linear_schedule_with_warmup
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report, accuracy_score, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
from tqdm.auto import tqdm
import os
import pandas as pd


class PromptBasedFakeNewsClassifier:
    """Fine-tune PhoBERT cho Fake News Detection"""

    def __init__(
        self,
        model_name: str = "vinai/phobert-base-v2",
        max_length: int = 256,
        batch_size: int = 16,
        learning_rate: float = 2e-5,
        num_epochs: int = 4,
        gradient_accumulation_steps: int = 2
    ):
        self.model_name = model_name
        self.max_length = max_length
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs
        self.gradient_accumulation_steps = gradient_accumulation_steps

        # Initialize
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForMaskedLM.from_pretrained(model_name)
        self.model.to(self.device)

        self.normalizer = VietnameseTextNormalizer()

        # Label token IDs
        self.label_tokens = {0: "th·∫≠t", 1: "gi·∫£"}
        self.label_token_ids = {
            label: self.tokenizer.convert_tokens_to_ids(token)
            for label, token in self.label_tokens.items()
        }

        print(f"Label token IDs: {self.label_token_ids}")

    def prepare_data(self, merged_df: pd.DataFrame, val_size: float = 0.15, test_size: float = 0.15):
        """
        Chu·∫©n b·ªã train/val/test datasets

        Args:
            merged_df: Merged dataframe
            val_size: Validation set ratio (default 0.15 = 15%)
            test_size: Test set ratio (default 0.15 = 15%)
            -> Train set will be 70%
        """
        # Step 1: Split train+val vs test (70:15:15)
        train_val_df, test_df = train_test_split(
            merged_df,
            test_size=test_size,
            stratify=merged_df['label'],
            random_state=42
        )

        # Step 2: Split train vs val
        val_ratio = val_size / (1 - test_size)  # Adjust ratio
        train_df, val_df = train_test_split(
            train_val_df,
            test_size=val_ratio,
            stratify=train_val_df['label'],
            random_state=42
        )

        print(f"\n{'='*50}")
        print(f"DATASET SPLIT")
        print(f"{'='*50}")
        print(f"Train set: {len(train_df)} samples ({len(train_df)/len(merged_df)*100:.1f}%)")
        print(f"Val set:   {len(val_df)} samples ({len(val_df)/len(merged_df)*100:.1f}%)")
        print(f"Test set:  {len(test_df)} samples ({len(test_df)/len(merged_df)*100:.1f}%)")

        # Calculate class weights (based on training set only)
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(train_df['label']),
            y=train_df['label']
        )
        self.class_weights = torch.FloatTensor(class_weights).to(self.device)

        print(f"\nClass weights: {class_weights}")
        print(f"\nLabel distribution:")
        print(f"  Train - REAL: {(train_df['label']==0).sum()}, FAKE: {(train_df['label']==1).sum()}")
        print(f"  Val   - REAL: {(val_df['label']==0).sum()}, FAKE: {(val_df['label']==1).sum()}")
        print(f"  Test  - REAL: {(test_df['label']==0).sum()}, FAKE: {(test_df['label']==1).sum()}")

        # Create datasets
        train_dataset = VietnameseFakeNewsDataset(
            train_df.reset_index(drop=True),
            self.tokenizer,
            self.normalizer,
            self.max_length
        )

        val_dataset = VietnameseFakeNewsDataset(
            val_df.reset_index(drop=True),
            self.tokenizer,
            self.normalizer,
            self.max_length
        )

        test_dataset = VietnameseFakeNewsDataset(
            test_df.reset_index(drop=True),
            self.tokenizer,
            self.normalizer,
            self.max_length
        )

        # Create dataloaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False
        )

        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False
        )

        self.test_loader = DataLoader(
            test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False
        )

        return train_dataset, val_dataset, test_dataset

    def train(self):
        """Training loop v·ªõi weighted loss"""
        # Optimizer & Scheduler
        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)

        total_steps = len(self.train_loader) * self.num_epochs // self.gradient_accumulation_steps
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(0.1 * total_steps),
            num_training_steps=total_steps
        )

        # Loss function with class weights
        criterion = nn.CrossEntropyLoss(weight=self.class_weights)

        best_f1 = 0.0
        history = {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_acc': []}

        for epoch in range(self.num_epochs):
            print(f"\n{'='*50}")
            print(f"Epoch {epoch + 1}/{self.num_epochs}")
            print(f"{'='*50}")

            # Training phase
            self.model.train()
            train_loss = 0
            optimizer.zero_grad()

            progress_bar = tqdm(self.train_loader, desc="Training")

            for step, batch in enumerate(progress_bar):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                mask_positions = batch['mask_position']
                labels = batch['labels'].to(self.device)

                # Forward pass
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                # Extract logits at mask positions
                logits = outputs.logits
                batch_size = logits.size(0)
                mask_logits = logits[torch.arange(batch_size), mask_positions, :]

                # Calculate loss only for label tokens
                label_logits = mask_logits[:, [self.label_token_ids[0], self.label_token_ids[1]]]

                # Create target: 0 for "th·∫≠t", 1 for "gi·∫£"
                target = (labels == self.label_token_ids[1]).long()

                loss = criterion(label_logits, target)
                loss = loss / self.gradient_accumulation_steps

                # Backward pass
                loss.backward()

                if (step + 1) % self.gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()

                train_loss += loss.item() * self.gradient_accumulation_steps
                progress_bar.set_postfix({'loss': train_loss / (step + 1)})

            avg_train_loss = train_loss / len(self.train_loader)
            print(f"Average training loss: {avg_train_loss:.4f}")

            # Validation phase
            val_metrics = self.evaluate(self.val_loader, split_name="Validation")

            print(f"Validation Loss: {val_metrics['loss']:.4f}")
            print(f"Validation Accuracy: {val_metrics['accuracy']:.4f}")
            print(f"Validation F1-Macro: {val_metrics['f1_macro']:.4f}")

            # Save history
            history['train_loss'].append(avg_train_loss)
            history['val_loss'].append(val_metrics['loss'])
            history['val_f1'].append(val_metrics['f1_macro'])
            history['val_acc'].append(val_metrics['accuracy'])

            # Save best model
            if val_metrics['f1_macro'] > best_f1:
                best_f1 = val_metrics['f1_macro']
                self.save_model("best_phobert_fakenews")
                print(f"‚úì Best model saved with F1: {best_f1:.4f}")

        # Save training history
        self.history = history
        print(f"\n{'='*50}")
        print(f"Training completed!")
        print(f"Best Validation F1-Macro: {best_f1:.4f}")
        print(f"{'='*50}")

        return history

    def evaluate(self, dataloader, split_name="Validation"):
        """
        Evaluation v·ªõi ƒë·∫ßy ƒë·ªß metrics: Accuracy, F1, Precision, Recall
        """
        self.model.eval()
        all_preds = []
        all_labels = []
        total_loss = 0

        criterion = nn.CrossEntropyLoss(weight=self.class_weights)

        with torch.no_grad():
            for batch in tqdm(dataloader, desc=f"Evaluating {split_name}"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                mask_positions = batch['mask_position']
                labels = batch['labels'].to(self.device)
                label_classes = batch['label_class']

                # Forward pass
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                logits = outputs.logits
                batch_size = logits.size(0)
                mask_logits = logits[torch.arange(batch_size), mask_positions, :]

                label_logits = mask_logits[:, [self.label_token_ids[0], self.label_token_ids[1]]]

                # Predictions
                preds = torch.argmax(label_logits, dim=1)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(label_classes.numpy())

                # Loss
                target = (labels == self.label_token_ids[1]).long()
                loss = criterion(label_logits, target)
                total_loss += loss.item()

        # Calculate metrics
        accuracy = accuracy_score(all_labels, all_preds)
        f1_macro = f1_score(all_labels, all_preds, average='macro')
        f1_weighted = f1_score(all_labels, all_preds, average='weighted')

        print(f"\n{split_name} Classification Report:")
        print(classification_report(all_labels, all_preds, target_names=['REAL', 'FAKE'], digits=4))

        # Confusion Matrix
        cm = confusion_matrix(all_labels, all_preds)
        print(f"\n{split_name} Confusion Matrix:")
        print(f"                Predicted")
        print(f"              REAL  FAKE")
        print(f"Actual REAL   {cm[0][0]:4d}  {cm[0][1]:4d}")
        print(f"       FAKE   {cm[1][0]:4d}  {cm[1][1]:4d}")

        avg_loss = total_loss / len(dataloader)

        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'predictions': all_preds,
            'labels': all_labels,
            'confusion_matrix': cm
        }

    def test(self):
        """
        ƒê√°nh gi√° tr√™n test set (ch·ªâ g·ªçi SAU KHI train xong)
        """
        print(f"\n{'='*60}")
        print(f"FINAL TEST SET EVALUATION")
        print(f"{'='*60}")

        test_metrics = self.evaluate(self.test_loader, split_name="Test")

        print(f"\n{'='*60}")
        print(f"FINAL TEST RESULTS:")
        print(f"{'='*60}")
        print(f"Test Accuracy:   {test_metrics['accuracy']:.4f}")
        print(f"Test F1-Macro:   {test_metrics['f1_macro']:.4f}")
        print(f"Test F1-Weighted: {test_metrics['f1_weighted']:.4f}")
        print(f"{'='*60}")

        return test_metrics

    def save_model(self, output_dir: str):
        """L∆∞u model v√† tokenizer"""
        os.makedirs(output_dir, exist_ok=True)
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)

        # Save config
        import json
        config = {
            'model_name': self.model_name,
            'max_length': self.max_length,
            'label_token_ids': self.label_token_ids,
            'label_tokens': self.label_tokens
        }
        with open(os.path.join(output_dir, 'training_config.json'), 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)

        print(f"Model saved to {output_dir}/")


# Example usage
def train_pipeline(dataset1_path: str, dataset2_path: str):
    """Main training pipeline v·ªõi train/val/test split"""

    # Step 1: Merge datasets
    print("Merging datasets...")
    merged_df = merge_datasets(dataset1_path, dataset2_path)
    print(f"Total samples: {len(merged_df)}")
    print(f"Label distribution:\n{merged_df['label'].value_counts()}")

    # Step 2: Initialize classifier
    classifier = PromptBasedFakeNewsClassifier(
        model_name="vinai/phobert-base-v2",
        max_length=256,
        batch_size=16,
        learning_rate=2e-5,
        num_epochs=4,
        gradient_accumulation_steps=2
    )

    # Step 3: Prepare data (70% train, 15% val, 15% test)
    classifier.prepare_data(merged_df, val_size=0.15, test_size=0.15)

    # Step 4: Train
    history = classifier.train()

    # Step 5: Test on held-out test set
    test_metrics = classifier.test()

    return classifier, history, test_metrics


# Run training
classifier, history, test_metrics = train_pipeline("dataset1.csv", "dataset2.csv")

# V·∫Ω Confusion Matrix ƒë∆°n gi·∫£n
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# L·∫•y predictions v√† labels t·ª´ test_metrics
y_true = test_metrics['labels']
y_pred = test_metrics['predictions']

# T·∫°o confusion matrix
cm = confusion_matrix(y_true, y_pred)

# V·∫Ω
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['REAL', 'FAKE'],
            yticklabels=['REAL', 'FAKE'],
            cbar=True,
            square=True,
            linewidths=1)

plt.title('Test Set - Confusion Matrix', fontsize=14, fontweight='bold')
plt.ylabel('Actual Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.tight_layout()

# L∆∞u file
plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
print("‚úÖ Saved: confusion_matrix.png")

plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %pip install onnx onnxruntime onnxscript
# %pip install optimum[onnxruntime]

# Block 5: ONNX Export (FIXED - Working Version)
import torch
import os
import json

try:
    import onnx
    from onnxruntime.quantization import quantize_dynamic, QuantType
    ONNX_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è  ONNX not available: {e}")
    ONNX_AVAILABLE = False

from transformers import AutoTokenizer, AutoModelForMaskedLM


class ONNXExporter:
    """Xu·∫•t model sang ONNX v√† quantization"""

    def __init__(self, model_path: str):
        if not ONNX_AVAILABLE:
            raise ImportError("Install: pip install onnx onnxruntime")

        self.model_path = model_path
        self.device = torch.device('cpu')

        print(f"Loading model from: {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForMaskedLM.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()

    def export_to_onnx(self, output_path: str = "model.onnx"):
        """Export to ONNX - FIXED version"""

        print(f"\n{'='*60}")
        print(f"EXPORTING TO ONNX")
        print(f"{'='*60}")

        # Dummy input
        dummy_text = "B√†i vi·∫øt n√†y l√† <mask> . Ti√™u_ƒë·ªÅ : Tin_t·ª©c . N·ªôi_dung : Ki·ªÉm_tra ."
        inputs = self.tokenizer(
            dummy_text,
            return_tensors="pt",
            max_length=256,
            padding="max_length",
            truncation=True
        )

        input_ids = inputs['input_ids'].to(self.device)
        attention_mask = inputs['attention_mask'].to(self.device)

        # Export v·ªõi opset 18 (recommended)
        print(f"Exporting to: {output_path}")

        try:
            torch.onnx.export(
                self.model,
                (input_ids, attention_mask),
                output_path,
                input_names=['input_ids', 'attention_mask'],
                output_names=['logits'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size', 1: 'sequence'},
                    'attention_mask': {0: 'batch_size', 1: 'sequence'},
                    'logits': {0: 'batch_size', 1: 'sequence'}
                },
                opset_version=18,  # Changed from 14 to 18
                do_constant_folding=True,
                export_params=True
            )

            # Verify
            onnx_model = onnx.load(output_path)
            onnx.checker.check_model(onnx_model)

            size_mb = os.path.getsize(output_path) / (1024 * 1024)
            print(f"‚úÖ ONNX export successful!")
            print(f"   Size: {size_mb:.2f} MB")

            return output_path

        except Exception as e:
            print(f"‚ùå Export failed: {e}")
            raise

    def quantize_onnx(self, onnx_path: str, quantized_path: str = "model_quantized.onnx"):
        """Quantize to INT8 - FIXED parameters"""

        print(f"\n{'='*60}")
        print(f"QUANTIZING MODEL")
        print(f"{'='*60}")

        try:
            # FIXED: Remove optimize_model parameter
            quantize_dynamic(
                model_input=onnx_path,
                model_output=quantized_path,
                weight_type=QuantType.QInt8
                # optimize_model parameter not supported in this version
            )

            original_size = os.path.getsize(onnx_path) / (1024 * 1024)
            quantized_size = os.path.getsize(quantized_path) / (1024 * 1024)
            compression = (1 - quantized_size/original_size) * 100

            print(f"‚úÖ Quantization successful!")
            print(f"   Original:  {original_size:.2f} MB")
            print(f"   Quantized: {quantized_size:.2f} MB")
            print(f"   Saved:     {original_size - quantized_size:.2f} MB ({compression:.1f}%)")

            if quantized_size < 150:
                print(f"   ‚úÖ Browser-ready (<150MB)")

            return quantized_path

        except Exception as e:
            print(f"‚ùå Quantization failed: {e}")
            raise

    def export_full_pipeline(self, output_dir: str = "onnx_models"):
        """Full export pipeline"""

        os.makedirs(output_dir, exist_ok=True)

        print(f"\n{'='*60}")
        print(f"ONNX EXPORT PIPELINE")
        print(f"{'='*60}")

        # Export to ONNX
        onnx_path = os.path.join(output_dir, "phobert_fakenews_fp32.onnx")
        self.export_to_onnx(onnx_path)

        # Quantize
        quantized_path = os.path.join(output_dir, "phobert_fakenews_int8.onnx")
        self.quantize_onnx(onnx_path, quantized_path)

        # Save tokenizer
        print(f"\nSaving tokenizer...")
        self.tokenizer.save_pretrained(output_dir)

        # Save metadata
        metadata = {
            'model': 'vinai/phobert-base-v2',
            'task': 'fake_news_detection',
            'labels': {0: 'REAL', 1: 'FAKE'},
            'label_tokens': {'th·∫≠t': 520, 'gi·∫£': 1163},
            'max_length': 256,
            'files': {
                'fp32': 'phobert_fakenews_fp32.onnx',
                'int8': 'phobert_fakenews_int8.onnx (recommended)'
            }
        }

        with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        print(f"\n{'='*60}")
        print(f"‚úÖ EXPORT COMPLETED!")
        print(f"{'='*60}")
        print(f"\nFiles in {output_dir}:")
        for file in os.listdir(output_dir):
            fpath = os.path.join(output_dir, file)
            if os.path.isfile(fpath):
                size = os.path.getsize(fpath) / (1024 * 1024)
                print(f"  {file:<45} {size:>8.2f} MB")

        print(f"\nüöÄ Use for browser: {quantized_path}")

        return quantized_path


# Export
def export_model_for_browser(trained_model_path: str = "best_phobert_fakenews"):
    """Export model for browser"""

    if not ONNX_AVAILABLE:
        print("‚ùå Install: pip install onnx onnxruntime onnxscript")
        return None

    try:
        exporter = ONNXExporter(trained_model_path)
        return exporter.export_full_pipeline("onnx_models")
    except Exception as e:
        print(f"‚ùå Failed: {e}")
        return None


# Run
quantized_model = export_model_for_browser("best_phobert_fakenews")

# 3. Download files v·ªÅ m√°y
from google.colab import files
!zip -r onnx_models.zip onnx_models/
files.download('onnx_models.zip')

# Block 6: ONNX Runtime Inference v·ªõi Text Chunking Strategy (FIXED)
import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer
import re


class ONNXFakeNewsDetector:
    """
    Inference engine v·ªõi Text Chunking ƒë·ªÉ x·ª≠ l√Ω documents d√†i
    T√≠ch h·ª£p s·∫µn cho RAG pipeline sau n√†y
    """

    def __init__(self, model_path: str, tokenizer_path: str, max_length: int = 256):
        # Load ONNX model
        self.session = ort.InferenceSession(
            model_path,
            providers=['CPUExecutionProvider']
        )

        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        self.normalizer = VietnameseTextNormalizer()
        self.max_length = max_length

        # Label mapping
        self.label_token_ids = {
            0: self.tokenizer.convert_tokens_to_ids("th·∫≠t"),
            1: self.tokenizer.convert_tokens_to_ids("gi·∫£")
        }

        print(f"‚úÖ Model loaded: {model_path}")
        print(f"   Max length: {max_length}")
        print(f"   Label tokens: {self.label_token_ids}")

    def smart_truncate(self, text: str, max_chars: int = 800) -> str:
        """
        Truncate th√¥ng minh: gi·ªØ ƒë·∫ßu v√† cu·ªëi, b·ªè gi·ªØa
        Chi·∫øn l∆∞·ª£c: First 60% + Last 40% c·ªßa max_chars
        """
        if len(text) <= max_chars:
            return text

        # Split ƒëi·ªÉm
        first_part_len = int(max_chars * 0.6)
        last_part_len = int(max_chars * 0.4)

        first_part = text[:first_part_len]
        last_part = text[-last_part_len:]

        return first_part + " [...] " + last_part

    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 100):
        """
        Chia text th√†nh chunks v·ªõi overlap
        D√πng cho RAG retrieval sau n√†y
        """
        if len(text) <= chunk_size:
            return [text]

        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]

            # T√¨m sentence boundary g·∫ßn nh·∫•t
            if end < len(text):
                # T√¨m d·∫•u c√¢u g·∫ßn nh·∫•t
                for sep in ['. ', '! ', '? ', '\n']:
                    last_sep = chunk.rfind(sep)
                    if last_sep > chunk_size * 0.7:  # √çt nh·∫•t 70% chunk
                        chunk = chunk[:last_sep + 1]
                        break

            chunks.append(chunk.strip())
            start = end - overlap if end < len(text) else end

        return chunks

    def preprocess(self, title: str, content: str, use_chunking: bool = False) -> dict:
        """
        Ti·ªÅn x·ª≠ l√Ω v·ªõi strategy ch·ªëng truncate
        """
        # Normalize
        title_norm = self.normalizer.normalize(title, preserve_mask=True)
        content_norm = self.normalizer.normalize(content, preserve_mask=True)

        # Word segmentation
        title_seg = self.normalizer.word_segment(title_norm) if title_norm else ""

        # CRITICAL FIX: Smart truncate content tr∆∞·ªõc khi segment
        if use_chunking:
            # Strategy 1: Multiple chunks (cho RAG sau n√†y)
            content_chunks = self.chunk_text(content_norm, chunk_size=500)
            content_seg = self.normalizer.word_segment(content_chunks[0])  # D√πng chunk ƒë·∫ßu
        else:
            # Strategy 2: Smart truncate (RECOMMENDED cho realtime)
            content_trunc = self.smart_truncate(content_norm, max_chars=600)
            content_seg = self.normalizer.word_segment(content_trunc)

        # Create prompt - ƒê·∫∂T <mask> ·ªû ƒê·∫¶U
        mask_token = self.tokenizer.mask_token
        prompt = f"B√†i vi·∫øt n√†y l√† {mask_token} . Ti√™u_ƒë·ªÅ : {title_seg} . N·ªôi_dung : {content_seg}"

        # Tokenize
        inputs = self.tokenizer(
            prompt,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='np'
        )

        # Verify mask exists
        mask_token_id = self.tokenizer.mask_token_id
        if mask_token_id not in inputs['input_ids'][0]:
            raise ValueError(
                f"‚ùå CRITICAL: Mask token lost during tokenization!\n"
                f"Prompt length: {len(prompt)} chars\n"
                f"After tokenization: {inputs['input_ids'].shape[1]} tokens\n"
                f"Tip: Reduce content length or increase max_length"
            )

        return inputs

    def predict(self, title: str, content: str, threshold: float = 0.5) -> dict:
        """
        D·ª± ƒëo√°n tin gi·∫£ v·ªõi confidence threshold
        """
        try:
            # Preprocess
            inputs = self.preprocess(title, content, use_chunking=False)

            # Run inference
            outputs = self.session.run(
                None,
                {
                    'input_ids': inputs['input_ids'].astype(np.int64),
                    'attention_mask': inputs['attention_mask'].astype(np.int64)
                }
            )

            logits = outputs[0]  # [batch, seq_len, vocab_size]

            # Find mask position
            mask_token_id = self.tokenizer.mask_token_id
            mask_pos = np.where(inputs['input_ids'][0] == mask_token_id)[0][0]

            # Extract logits at mask
            mask_logits = logits[0, mask_pos, :]

            # Get label logits
            label_logits = mask_logits[[self.label_token_ids[0], self.label_token_ids[1]]]

            # Softmax
            exp_logits = np.exp(label_logits - np.max(label_logits))  # Numerical stability
            probs = exp_logits / np.sum(exp_logits)

            prediction = int(np.argmax(probs))
            confidence = float(probs[prediction])

            # Apply threshold
            if confidence < threshold:
                label = 'UNCERTAIN'
                warning = f'Low confidence ({confidence:.3f} < {threshold})'
            else:
                label = 'FAKE' if prediction == 1 else 'REAL'
                warning = None

            result = {
                'prediction': label,
                'confidence': confidence,
                'probabilities': {
                    'REAL': float(probs[0]),
                    'FAKE': float(probs[1])
                },
                'warning': warning
            }

            return result

        except Exception as e:
            return {
                'prediction': 'ERROR',
                'confidence': 0.0,
                'error': str(e),
                'probabilities': {'REAL': 0.0, 'FAKE': 0.0}
            }


# ===== USAGE EXAMPLES =====

def test_inference_with_long_text():
    """Test v·ªõi text d√†i nh∆∞ TikTok transcript"""

    detector = ONNXFakeNewsDetector(
        model_path="onnx_models/phobert_fakenews_int8.onnx",
        tokenizer_path="onnx_models",
        max_length=256
    )

    print("\n" + "="*70)
    print("TESTING WITH LONG DOCUMENTS")
    print("="*70)

    # Test 1: Short text
    print("\n[Test 1: Short Text]")
    result1 = detector.predict(
        title="N∆∞·ªõc l≈© ng·∫≠p t·ªõi n√≥c nh√†",
        content="Ng∆∞·ªùi d√¢n ph·∫£i leo l√™n m√°i nh√† ch·ªù c·ª©u h·ªô khi n∆∞·ªõc d√¢ng l√™n."
    )
    print(f"Result: {result1['prediction']} ({result1['confidence']:.3f})")
    print(f"Probs: REAL={result1['probabilities']['REAL']:.3f}, FAKE={result1['probabilities']['FAKE']:.3f}")

    # Test 2: Long text (TikTok transcript)
    print("\n[Test 2: Long Text - TikTok Transcript]")
    long_content = """
    D·ª± b√°o hai k·ªãch b·∫£n c·ªßa b√£o s·ªë 15 c√¥ t·ªì t√°c ƒë·ªông ƒë·∫øn n∆∞·ªõc ta.
    Hi·ªán bi·∫øn ƒë√¥ng ƒëang ch·ªãu t√°c ƒë·ªông c·ªßa b√£o s·ªë 15 c√≥ t√™n qu·ªëc t·∫ø l√† c√¥ t·ªì.
    √â l√† c∆°n b√£o c√≥ nh·ªØng bi·∫øn kh√≥ l∆∞·ªùng v√¨ ƒëang b·ªã chi ph·ªëi b·ªüi nhi·ªÅu y·∫øu t·ªë th·ªùi ti·∫øt b·∫•t l·ª£i,
    khi·∫øn ƒë∆∞·ªùng ƒëi v√† c∆∞·ªùng ƒë·ªô c·ªßa b√£o thay ƒë·ªïi li√™n t·ª•c. √îng Mai Van Kim gi√°m ƒë·ªëc trung t√¢m
    d·ª± b√°o kh√≠ t∆∞·ª£ng th·ªßy vƒÉn qu·ªëc gia cho bi·∫øt. Hi·ªán b√£o s·ªë 15 c√¥ t·ªì ƒëang ƒë·ªïi h∆∞·ªõng di chuy·ªÉn
    theo h∆∞·ªõng T√¢y T√¢y B·∫Øc. Th·ª±c ƒë√≥ l√† h∆∞·ªõng T√¢y B·∫Øc ƒë·ªìng th·ªùi tƒÉng c·∫•p v√† c√≥ kh·∫£ nƒÉng x·∫£y ra
    hai k·ªãch b·∫£n. V·ªõi k·ªãch b·∫£n ch√≠nh s√°t xu·∫•t 70% b√£o s·ªë 15 ƒë·ªïi h∆∞·ªõng khi c√°ch kh√°nh hoa gia lai
    kho·∫£ng 40-50 ƒë·∫øn 500 kHz v√†o kho·∫£ng ng√†y 30-11. Gi·ªõi h·∫°n tr√™n n∆∞·ªõc li·ªÅn ƒë∆∞·ª£c ƒë√°nh gi√° l√†
    √≠t kh·∫£ nƒÉng x·∫£y ra m∆∞a c√≥ th·ªÉ xu·∫•t hi·ªán t·ª´ ƒë√† n·∫µng ƒë·∫øn b√¨nh ƒë·ªãnh trong ng√†y 1 ƒë·∫øn 2-12.
    """

    result2 = detector.predict(
        title="D·ª± b√°o hai k·ªãch b·∫£n c·ªßa B√£o s·ªë 15",
        content=long_content
    )
    print(f"Result: {result2['prediction']} ({result2['confidence']:.3f})")
    print(f"Probs: REAL={result2['probabilities']['REAL']:.3f}, FAKE={result2['probabilities']['FAKE']:.3f}")
    if result2.get('warning'):
        print(f"‚ö†Ô∏è  {result2['warning']}")

    # Test 3: Very long text (stress test)
    print("\n[Test 3: Very Long Text - Stress Test]")
    very_long = long_content * 5  # 5x longer
    result3 = detector.predict(
        title="Test text c·ª±c d√†i",
        content=very_long
    )
    print(f"Result: {result3['prediction']} ({result3['confidence']:.3f})")
    print(f"Content length: {len(very_long)} chars")

    # Test 4:
    print("\n[Test 4: Tin Th·∫≠t c·ªßa 60s]")
    result4 = detector.predict(
        title="XEM V√åDEO M·ªöI TH·∫§Y ƒê√ÅM CH√ÅY QU√Å KH·ª¶NG KHI·∫æP #theanh28 #60giay #tiktoknews ",
        content="Video quay l·∫°i v·ª• ch√°y 8 t√≤a chung c∆∞ t·∫°i H√¥ng Kong th·∫≠t s·ª± khung c·∫£nh qu√° th·∫£m kh·ªëc nh∆∞ ng√†y t·∫≠n th·∫ø ƒë·∫ßy √°m ·∫£nh"
    )
    print(f"Result: {result4['prediction']} ({result4['confidence']:.3f})")
    print(f"Probs: REAL={result4['probabilities']['REAL']:.3f}, FAKE={result4['probabilities']['FAKE']:.3f}")

    # Test 5:
    print("\n[Test 5: Tin gi·∫£]")
    result5 = detector.predict(
        title="TIN VUI C·ª∞C L·ªöN: CH√çNH PH·ª¶ CHI 100.000 T·ª∂ T·∫∂NG TI·ªÄN M·∫∂T CHO D√ÇN VUI T·∫æT NGUY√äN ƒê√ÅN#tintuc #tintuc24h #xuhuong",
        content="Tinh c·ª±c vui l√†m tr·∫•n ƒë·ªông c·∫£ n∆∞·ªõc, th·ªß t∆∞·ªõng ph·∫°m minh ch√≠nh v·ª´a ra c√¥ng vi·ªác, ch√≠nh ph·ªß quy·∫øt ƒë·ªãnh chi 100.000 t·ª∑ ƒë·ªông, ƒë·ªÉ Lisi to√†n d√¢n d·ªãp t·∫øt mi√™n ƒë√°ng 2026. Theo ƒë√≥, bu·ªïi ng∆∞·ªùi d√¢n Vi·ªát Nam s·∫Ω ƒë∆∞·ª£c nh·∫≠n m·ªôt chi·ªáu ƒë·ªìng ti·ªÅn m·∫∑t ph√°t t·ª´ ng√†y 15.25 th√°ng tr·∫°p. ƒê√¢y ƒë∆∞·ª£c coi l√† m√≥n qu·∫£ kh·ªßng ch∆∞a t·ª´ng c√≥ trong l·ªãch s·ª≠, sau khi tr∆∞·ªõc ƒë√≥, m·ªói ng∆∞·ªùi t√¨nh ƒë∆∞·ª£c Lisi 100.000 ƒë·ªìng d·ªãp 2 th√°ng chi·∫øn. Ch√≠nh s√°ch n√†y ngay l·∫≠p t·ª©c nh·∫≠n ƒë∆∞·ª£c s·ª± ·ªßng h·ªô nhi·ªát li·ªát, d√¢n lao ƒë·ªông vui m·ª´ng, th√¨ tr∆∞·ªùng suy ƒë·ªông, ai c≈©ng c√≥ t·∫øt tr·ªçng v·∫πp. Nh∆∞ng c√¥ng d·ªÖ l√™n nhi·ªÅu c√¢u h·ªèi, li·ªáu ƒë√¢y l√† b·∫•t ngo·∫°ch n√¢ng cao ƒë·ªùi s·ªëng, hay l√† li·ªáu thu·ªëc k√≠ch c·∫ßu ƒë·ªÉ ƒë·ªëi ph√≥ l·∫°ng ph√°t."
    )
    print(f"Result: {result5['prediction']} ({result5['confidence']:.3f})")
    print(f"Probs: REAL={result5['probabilities']['REAL']:.3f}, FAKE={result5['probabilities']['FAKE']:.3f}")

    print("\n" + "="*70)
    print("‚úÖ All tests completed!")
    print("="*70)


# Run test
test_inference_with_long_text()