{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MngMks6vnLY5",
        "outputId": "d0b6565a-06a7-4091-efd6-ad2d94b53214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ydGsnWImiVh",
        "outputId": "3e54c6f1-8fd0-4f3b-9373-f58af9c16ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ ƒêang x·ª≠ l√Ω v√† g·ªôp d·ªØ li·ªáu...\n",
            "üìä T·ªïng s·ªë d√≤ng th√¥: 9939\n",
            "üßπ ƒêang l√†m s·∫°ch text...\n",
            "‚ú® ƒê√£ x√≥a 270 d√≤ng tr√πng l·∫∑p.\n",
            "\n",
            "========================================\n",
            "‚úÖ XONG! File cu·ªëi c√πng: FINAL_DATASET_MERGED.csv\n",
            "üëâ S·ªë l∆∞·ª£ng m·∫´u s·∫°ch: 9375\n",
            "üëâ M·∫´u d·ªØ li·ªáu:\n",
            "                                                text  label\n",
            "0  ƒê·∫æN 18H CHI·ªÄU 10/4 : TH√äM 2 CA M·∫ÆC COVID-19 BN...      0\n",
            "1  Th·∫ø gi·ªõi tu·∫ßn qua: N·ª£ c√¥ng M·ªπ tƒÉng kinh ho√†ng ...      0\n",
            "2  S·ª± c·ªë ƒë·ª©t c√°p AAG kh√¥ng ·∫£nh h∆∞·ªüng t·ªõi ch·∫•t l∆∞·ª£...      0\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "import unicodedata\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. C·∫§U H√åNH INPUT (ƒêi·ªÅn t·∫•t c·∫£ ƒë∆∞·ªùng d·∫´n file v√†o ƒë√¢y)\n",
        "# ==============================================================================\n",
        "ALL_FILES = [\n",
        "    # --- Nh√≥m 3 file c≈© (Social) ---\n",
        "    \"/content/train.csv\",\n",
        "    \"/content/val.csv\",\n",
        "    \"/content/test.csv\",\n",
        "\n",
        "    # --- File m·ªõi (News) ---\n",
        "    \"/content/vn_news_226_tlfr.csv\"\n",
        "]\n",
        "\n",
        "OUTPUT_FILE = \"FINAL_DATASET_MERGED.csv\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. H√ÄM CLEANING (CHU·∫®N H√ìA & GI·ªÆ NG√ÄY TH√ÅNG)\n",
        "# ==============================================================================\n",
        "def smart_clean(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "\n",
        "    # 1. Fix l·ªói font (Unicode NFC) & X√≥a xu·ªëng d√≤ng\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = text.replace(\"_\", \" \").replace(\"\\n\", \" . \")\n",
        "\n",
        "    # 2. X√≥a Emoji & URL\n",
        "    text = emoji.replace_emoji(text, replace=\"\")\n",
        "    text = re.sub(r'(https?://\\S+|www\\.\\S+|<URL>)', '', text)\n",
        "\n",
        "    # 3. C·∫Øt r√°c cu·ªëi c√¢u (Xem th√™m, Ngu·ªìn...)\n",
        "    text = re.sub(r'(Xem th√™m.*|Theo .*|Ngu·ªìn:.*|·∫¢nh:.*|Cre:.*)$', '', text, flags=re.I|re.M)\n",
        "\n",
        "    # 4. GI·ªÆ: Ch·ªØ, S·ªë, D·∫•u c√¢u c∆° b·∫£n, D·∫•u / (ng√†y th√°ng)\n",
        "    text = re.sub(r'[^\\w\\s.,!?;:\\%\\-\\(\\)\\/]', ' ', text)\n",
        "\n",
        "    # 5. Fix l·ªói kho·∫£ng tr·∫Øng (10 / 4 -> 10/4, . . -> .)\n",
        "    text = re.sub(r'\\s*/\\s*', '/', text)\n",
        "    text = re.sub(r'\\.\\s*\\.', '.', text)\n",
        "\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. PIPELINE G·ªòP & X·ª¨ L√ù\n",
        "# ==============================================================================\n",
        "try:\n",
        "    print(\"üöÄ ƒêang x·ª≠ l√Ω v√† g·ªôp d·ªØ li·ªáu...\")\n",
        "    dfs = []\n",
        "\n",
        "    for f in ALL_FILES:\n",
        "        # ƒê·ªçc file (ch∆∞a l·ªçc c·ªôt v·ªôi ƒë·ªÉ check t√™n c·ªôt)\n",
        "        try:\n",
        "            temp_df = pd.read_csv(f, dtype=str) # ƒê·ªçc t·∫•t c·∫£ l√† string ƒë·ªÉ tr√°nh l·ªói\n",
        "\n",
        "            # --- T·ª∞ ƒê·ªòNG CHU·∫®N H√ìA T√äN C·ªòT ---\n",
        "            # N·∫øu th·∫•y 'post_message' th√¨ ƒë·ªïi th√†nh 'text'\n",
        "            if 'post_message' in temp_df.columns:\n",
        "                temp_df.rename(columns={'post_message': 'text'}, inplace=True)\n",
        "\n",
        "            # Ch·ªâ gi·ªØ l·∫°i 'text' v√† 'label', b·ªè qua n·∫øu file thi·∫øu c·ªôt\n",
        "            if 'text' in temp_df.columns and 'label' in temp_df.columns:\n",
        "                dfs.append(temp_df[['text', 'label']])\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è B·ªè qua file {f}: Kh√¥ng t√¨m th·∫•y c·ªôt 'text' ho·∫∑c 'label'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå L·ªói ƒë·ªçc file {f}: {e}\")\n",
        "\n",
        "    # G·ªôp t·∫•t c·∫£\n",
        "    if not dfs:\n",
        "        raise ValueError(\"Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c load!\")\n",
        "\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"üìä T·ªïng s·ªë d√≤ng th√¥: {len(df)}\")\n",
        "\n",
        "    # --- CLEANING & FILTERING ---\n",
        "    print(\"üßπ ƒêang l√†m s·∫°ch text...\")\n",
        "    df['text'] = df['text'].apply(smart_clean)\n",
        "\n",
        "    # X·ª≠ l√Ω c·ªôt Label (v·ªÅ 0 v√† 1)\n",
        "    df['label'] = pd.to_numeric(df['label'], errors='coerce').fillna(-1).astype(int)\n",
        "    df = df[df['label'].isin([0, 1])] # Ch·ªâ l·∫•y label 0 ho·∫∑c 1\n",
        "\n",
        "    # L·ªçc text r√°c (ng·∫Øn < 15 k√Ω t·ª±)\n",
        "    df = df[df['text'].str.len() > 15]\n",
        "\n",
        "    # X√≥a tr√πng l·∫∑p\n",
        "    before = len(df)\n",
        "    df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
        "    print(f\"‚ú® ƒê√£ x√≥a {before - len(df)} d√≤ng tr√πng l·∫∑p.\")\n",
        "\n",
        "    # --- L∆ØU FILE ---\n",
        "    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"‚úÖ XONG! File cu·ªëi c√πng: {OUTPUT_FILE}\")\n",
        "    print(f\"üëâ S·ªë l∆∞·ª£ng m·∫´u s·∫°ch: {len(df)}\")\n",
        "    print(f\"üëâ M·∫´u d·ªØ li·ªáu:\")\n",
        "    print(df.head(3))\n",
        "    print(\"=\"*40)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå CRITICAL ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_FILE = \"DATASET_FOR_EDA_RAW.csv\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. H√ÄM CLEAN NH·∫∏ (MINIMAL CLEAN)\n",
        "# ==============================================================================\n",
        "def raw_clean(text):\n",
        "    \"\"\"\n",
        "    Ch·ªâ s·ª≠a l·ªói k·ªπ thu·∫≠t, gi·ªØ nguy√™n n·ªôi dung g·ªëc (Emoji, Teencode...)\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str): return \"\"\n",
        "\n",
        "    # 1. Fix l·ªói v·ª° font ti·∫øng Vi·ªát (B·∫Øt bu·ªôc ph·∫£i l√†m ƒë·ªÉ m√°y ƒë·ªçc ƒë∆∞·ª£c ch·ªØ)\n",
        "    # V√≠ d·ª•: \"ƒëu ng\" (NFD) -> \"ƒë√∫ng\" (NFC)\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "\n",
        "    # 2. X√≥a d·∫•u g·∫°ch d∆∞·ªõi \"_\" do b·ªô t√°ch t·ª´ c≈© t·∫°o ra\n",
        "    # C√°i n√†y l√† nhi·ªÖu k·ªπ thu·∫≠t, kh√¥ng ph·∫£i do ng∆∞·ªùi d√πng vi·∫øt, n√™n x√≥a cho d·ªÖ ƒë·ªçc\n",
        "    # \"qu·ªëc_t·ªãch\" -> \"qu·ªëc t·ªãch\"\n",
        "    text = text.replace(\"_\", \" \")\n",
        "\n",
        "    # 3. X·ª≠ l√Ω xu·ªëng d√≤ng ƒë·ªÉ CSV kh√¥ng b·ªã l·ªói\n",
        "    # (T√πy ch·ªçn: N·∫øu mu·ªën gi·ªØ nguy√™n xu·ªëng d√≤ng trong √¥ Excel th√¨ b·ªè d√≤ng n√†y)\n",
        "    # Nh∆∞ng ƒë·ªÉ code Python ƒë·ªçc d·ªÖ th√¨ n√™n thay b·∫±ng kho·∫£ng tr·∫Øng ho·∫∑c d·∫•u ch·∫•m\n",
        "    # text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. PIPELINE G·ªòP D·ªÆ LI·ªÜU\n",
        "# ==============================================================================\n",
        "try:\n",
        "    print(\"üöÄ ƒêang g·ªôp d·ªØ li·ªáu ƒë·ªÉ l√†m EDA...\")\n",
        "    dfs = []\n",
        "\n",
        "    for f in ALL_FILES:\n",
        "        try:\n",
        "            # ƒê·ªçc file, √©p ki·ªÉu string ƒë·ªÉ kh√¥ng l·ªói\n",
        "            df_temp = pd.read_csv(f, dtype=str)\n",
        "\n",
        "            # ƒê·ªïi t√™n c·ªôt post_message th√†nh text n·∫øu c√≥\n",
        "            if 'post_message' in df_temp.columns:\n",
        "                df_temp.rename(columns={'post_message': 'text'}, inplace=True)\n",
        "\n",
        "            # Ch·ªâ l·∫•y 2 c·ªôt quan tr·ªçng\n",
        "            if 'text' in df_temp.columns and 'label' in df_temp.columns:\n",
        "                dfs.append(df_temp[['text', 'label']])\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è File {f} thi·∫øu c·ªôt text/label -> B·ªè qua.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå L·ªói ƒë·ªçc file {f}: {e}\")\n",
        "\n",
        "    # G·ªôp l·∫°i\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"üìä T·ªïng s·ªë d√≤ng thu th·∫≠p: {len(df)}\")\n",
        "\n",
        "    # --- X·ª¨ L√ù NH·∫∏ ---\n",
        "    # 1. Ch·∫°y h√†m clean nh·∫π\n",
        "    df['text'] = df['text'].apply(raw_clean)\n",
        "\n",
        "    # 2. X·ª≠ l√Ω Label v·ªÅ d·∫°ng s·ªë (0 v√† 1)\n",
        "    # √âp ki·ªÉu sang s·ªë, c√°i n√†o l·ªói th√†nh NaN r·ªìi x√≥a\n",
        "    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
        "    df = df.dropna(subset=['label']) # X√≥a d√≤ng kh√¥ng c√≥ nh√£n\n",
        "    df['label'] = df['label'].astype(int)\n",
        "\n",
        "    # 3. X√≥a c√°c d√≤ng text qu√° ng·∫Øn ho·∫∑c r·ªóng (NULL)\n",
        "    # ƒê·ªÉ EDA ch√≠nh x√°c th√¨ √≠t nh·∫•t ph·∫£i c√≥ ch·ªØ\n",
        "    df = df.dropna(subset=['text'])\n",
        "    df = df[df['text'].str.strip().str.len() > 0]\n",
        "\n",
        "    # 4. X√≥a tr√πng l·∫∑p (Tuy·ªát ƒë·ªëi tr√πng nhau)\n",
        "    # V·∫´n n√™n x√≥a tr√πng l·∫∑p ho√†n to√†n ƒë·ªÉ bi·ªÉu ƒë·ªì kh√¥ng b·ªã bias\n",
        "    before = len(df)\n",
        "    df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
        "    print(f\"‚ú® ƒê√£ x√≥a {before - len(df)} b√†i tr√πng l·∫∑p ho√†n to√†n.\")\n",
        "\n",
        "    # --- L∆ØU FILE ---\n",
        "    # utf-8-sig gi√∫p m·ªü b·∫±ng Excel tr√™n Windows kh√¥ng b·ªã l·ªói font ti·∫øng Vi·ªát\n",
        "    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"‚úÖ ƒê√É XU·∫§T FILE EDA: {OUTPUT_FILE}\")\n",
        "    print(f\"üëâ S·ªë l∆∞·ª£ng m·∫´u: {len(df)}\")\n",
        "    print(f\"üëâ D·ªØ li·ªáu m·∫´u (V·∫´n c√≤n Emoji):\")\n",
        "    print(df[['text', 'label']].head(3))\n",
        "    print(\"=\"*40)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå L·ªói: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAVHDUN9wGv1",
        "outputId": "19bf9fdf-0aec-40a4-c1b9-10c13c55ce10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ ƒêang g·ªôp d·ªØ li·ªáu ƒë·ªÉ l√†m EDA...\n",
            "üìä T·ªïng s·ªë d√≤ng thu th·∫≠p: 9939\n",
            "‚ú® ƒê√£ x√≥a 103 b√†i tr√πng l·∫∑p ho√†n to√†n.\n",
            "\n",
            "========================================\n",
            "‚úÖ ƒê√É XU·∫§T FILE EDA: DATASET_FOR_EDA_RAW.csv\n",
            "üëâ S·ªë l∆∞·ª£ng m·∫´u: 9836\n",
            "üëâ D·ªØ li·ªáu m·∫´u (V·∫´n c√≤n Emoji):\n",
            "                                                text  label\n",
            "0  ƒê·∫æN 18H CHI·ªÄU 10/4 : TH√äM 2 CA M·∫ÆC COVID-19 ‚û°Ô∏è...      0\n",
            "1  Th·∫ø gi·ªõi tu·∫ßn qua: N·ª£ c√¥ng M·ªπ tƒÉng kinh ho√†ng ...      0\n",
            "2  S·ª± c·ªë ƒë·ª©t c√°p AAG kh√¥ng ·∫£nh h∆∞·ªüng t·ªõi ch·∫•t l∆∞·ª£...      0\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. C·∫§U H√åNH INPUT\n",
        "# ==============================================================================\n",
        "FILE_DS1 = \"/content/FINAL_DATASET_MERGED.csv\"          # File News/Social\n",
        "FILE_DS2 = \"/content/dataset_TIKTOK_FAKE_NEWS_FINAL.csv\" # File TikTok\n",
        "OUTPUT_FILE = \"MERGED_DATASET_FINAL.csv\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. H√ÄM X·ª¨ L√ù (C√ì DEBUG)\n",
        "# ==============================================================================\n",
        "\n",
        "def process_dataset1(file_path):\n",
        "    print(f\"\\nüîπ ƒêang x·ª≠ l√Ω Dataset 1: {file_path}\")\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {file_path}. H√£y upload l·∫°i!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, dtype=str)\n",
        "        print(f\"   -> ƒê·ªçc ƒë∆∞·ª£c: {len(df)} d√≤ng ban ƒë·∫ßu.\")\n",
        "        print(f\"   -> C√°c c·ªôt hi·ªán c√≥: {list(df.columns)}\")\n",
        "\n",
        "        # T·ª± ƒë·ªông t√¨m c·ªôt n·ªôi dung (text ho·∫∑c post_message ho·∫∑c content)\n",
        "        col_map = {}\n",
        "        if 'text' in df.columns: col_map['text'] = 'content'\n",
        "        elif 'post_message' in df.columns: col_map['post_message'] = 'content'\n",
        "        elif 'content' in df.columns: pass # ƒê√£ ƒë√∫ng t√™n\n",
        "        else:\n",
        "            print(\"‚ùå L·ªói DS1: Kh√¥ng t√¨m th·∫•y c·ªôt n·ªôi dung (text/post_message)\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df.rename(columns=col_map, inplace=True)\n",
        "\n",
        "        # T·∫°o title r·ªóng n·∫øu ch∆∞a c√≥\n",
        "        if 'title' not in df.columns:\n",
        "            df['title'] = \"\"\n",
        "\n",
        "        # X·ª≠ l√Ω label\n",
        "        if 'label' not in df.columns:\n",
        "            print(\"‚ö†Ô∏è C·∫£nh b√°o DS1: Kh√¥ng c√≥ c·ªôt label, m·∫∑c ƒë·ªãnh g√°n = 0\")\n",
        "            df['label'] = 0\n",
        "\n",
        "        return df[['title', 'content', 'label']]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói x·ª≠ l√Ω DS1: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def process_dataset2(file_path):\n",
        "    print(f\"\\nüîπ ƒêang x·ª≠ l√Ω Dataset 2: {file_path}\")\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {file_path}. H√£y upload l·∫°i!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, dtype=str)\n",
        "        print(f\"   -> ƒê·ªçc ƒë∆∞·ª£c: {len(df)} d√≤ng ban ƒë·∫ßu.\")\n",
        "        print(f\"   -> C√°c c·ªôt hi·ªán c√≥: {list(df.columns)}\")\n",
        "\n",
        "        # Mapping TikTok\n",
        "        rename_map = {}\n",
        "        if 'desc' in df.columns: rename_map['desc'] = 'title'\n",
        "        if 'text' in df.columns: rename_map['text'] = 'content'\n",
        "\n",
        "        df.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "        # Ki·ªÉm tra c·ªôt b·∫Øt bu·ªôc\n",
        "        if 'content' not in df.columns:\n",
        "             print(\"‚ùå L·ªói DS2: Kh√¥ng t√¨m th·∫•y c·ªôt 'text' (ƒë√£ rename th√†nh content)\")\n",
        "             return pd.DataFrame()\n",
        "\n",
        "        # Map Label\n",
        "        if 'category' in df.columns:\n",
        "            df['category'] = df['category'].str.lower().str.strip()\n",
        "            label_map = {'fake': 1, 'real': 0, 'tin gi·∫£': 1, 'tin th·∫≠t': 0}\n",
        "            df['label'] = df['category'].map(label_map)\n",
        "            # Drop NaN label\n",
        "            before_drop = len(df)\n",
        "            df = df.dropna(subset=['label'])\n",
        "            print(f\"   -> ƒê√£ lo·∫°i b·ªè {before_drop - len(df)} d√≤ng kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c nh√£n (fake/real).\")\n",
        "        elif 'label' in df.columns:\n",
        "            print(\"   -> ƒê√£ c√≥ s·∫µn c·ªôt label.\")\n",
        "        else:\n",
        "            print(\"‚ùå L·ªói DS2: Kh√¥ng t√¨m th·∫•y c·ªôt 'category' ho·∫∑c 'label'\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df['title'] = df['title'].fillna(\"\")\n",
        "        return df[['title', 'content', 'label']]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói x·ª≠ l√Ω DS2: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. MAIN (CH·∫†Y TR·ª∞C TI·∫æP)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"üöÄ B·∫ÆT ƒê·∫¶U QU√Å TR√åNH G·ªòP...\")\n",
        "\n",
        "# 1. X·ª≠ l√Ω t·ª´ng file\n",
        "df1 = process_dataset1(FILE_DS1)\n",
        "df2 = process_dataset2(FILE_DS2)\n",
        "\n",
        "print(f\"\\nüìä T·ªïng k·∫øt tr∆∞·ªõc khi g·ªôp:\")\n",
        "print(f\"   - Dataset 1 s·∫°ch: {len(df1)} d√≤ng\")\n",
        "print(f\"   - Dataset 2 s·∫°ch: {len(df2)} d√≤ng\")\n",
        "\n",
        "# 2. G·ªôp\n",
        "if len(df1) == 0 and len(df2) == 0:\n",
        "    print(\"\\n‚ùå C·∫¢ 2 FILE ƒê·ªÄU R·ªñNG HO·∫∂C L·ªñI. D·ª™NG CH∆Ø∆†NG TR√åNH.\")\n",
        "else:\n",
        "    merged_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "    # 3. Format chu·∫©n\n",
        "    merged_df['label'] = pd.to_numeric(merged_df['label'], errors='coerce').fillna(0).astype(int)\n",
        "    merged_df['content'] = merged_df['content'].fillna(\"\")\n",
        "    merged_df['title'] = merged_df['title'].fillna(\"\")\n",
        "\n",
        "    # 4. X√≥a tr√πng l·∫∑p\n",
        "    before = len(merged_df)\n",
        "    merged_df.drop_duplicates(subset=['title', 'content'], keep='first', inplace=True)\n",
        "    print(f\"\\n‚ú® ƒê√£ x√≥a {before - len(merged_df)} d√≤ng tr√πng l·∫∑p khi g·ªôp.\")\n",
        "\n",
        "    # 5. L∆∞u\n",
        "    merged_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"‚úÖ HO√ÄN T·∫§T! FILE RA: {OUTPUT_FILE}\")\n",
        "    print(f\"üìä T·ªîNG S·ªê M·∫™U CU·ªêI C√ôNG: {len(merged_df)}\")\n",
        "    print(\"=\"*50)\n",
        "    print(merged_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW4qCx38KTlN",
        "outputId": "79717c58-7dcd-4951-cf79-f6d1384fbe1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ B·∫ÆT ƒê·∫¶U QU√Å TR√åNH G·ªòP...\n",
            "\n",
            "üîπ ƒêang x·ª≠ l√Ω Dataset 1: /content/FINAL_DATASET_MERGED.csv\n",
            "   -> ƒê·ªçc ƒë∆∞·ª£c: 9375 d√≤ng ban ƒë·∫ßu.\n",
            "   -> C√°c c·ªôt hi·ªán c√≥: ['text', 'label']\n",
            "\n",
            "üîπ ƒêang x·ª≠ l√Ω Dataset 2: /content/dataset_TIKTOK_FAKE_NEWS_FINAL.csv\n",
            "   -> ƒê·ªçc ƒë∆∞·ª£c: 2484 d√≤ng ban ƒë·∫ßu.\n",
            "   -> C√°c c·ªôt hi·ªán c√≥: ['url', 'text', 'category', 'desc', 'keyword', 'createTime', 'shareCount', 'commentCount', 'playCount', 'diggCount', 'collectCount', 'author_nickname', 'author_unique_id', 'author_id', 'video_id', 'thumnail_url']\n",
            "   -> ƒê√£ lo·∫°i b·ªè 0 d√≤ng kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c nh√£n (fake/real).\n",
            "\n",
            "üìä T·ªïng k·∫øt tr∆∞·ªõc khi g·ªôp:\n",
            "   - Dataset 1 s·∫°ch: 9375 d√≤ng\n",
            "   - Dataset 2 s·∫°ch: 2484 d√≤ng\n",
            "\n",
            "‚ú® ƒê√£ x√≥a 0 d√≤ng tr√πng l·∫∑p khi g·ªôp.\n",
            "\n",
            "==================================================\n",
            "‚úÖ HO√ÄN T·∫§T! FILE RA: MERGED_DATASET_FINAL.csv\n",
            "üìä T·ªîNG S·ªê M·∫™U CU·ªêI C√ôNG: 11859\n",
            "==================================================\n",
            "  title                                            content  label\n",
            "0        ƒê·∫æN 18H CHI·ªÄU 10/4 : TH√äM 2 CA M·∫ÆC COVID-19 BN...      0\n",
            "1        Th·∫ø gi·ªõi tu·∫ßn qua: N·ª£ c√¥ng M·ªπ tƒÉng kinh ho√†ng ...      0\n",
            "2        S·ª± c·ªë ƒë·ª©t c√°p AAG kh√¥ng ·∫£nh h∆∞·ªüng t·ªõi ch·∫•t l∆∞·ª£...      0\n",
            "3        TRI·ªÜU CH·ª®NG NHI·ªÑM CORONA QUA T·ª™NG NG√ÄY Ng√†y 1 ...      1\n",
            "4        b·ªã phong t·ªèa t·ª´ ng√†y 28-3.. Phong t·ªèa 14 ng√†y ...      1\n"
          ]
        }
      ]
    }
  ]
}