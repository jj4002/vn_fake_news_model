{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzK9kyE9g2ZY",
        "outputId": "e51c28b1-a537-4e71-9662-f0baea176692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "üöÄ B·∫ÆT ƒê·∫¶U L√ÄM S·∫†CH DATASET REAL\n",
            "üìä T·ªïng s·ªë d√≤ng ban ƒë·∫ßu: 1590\n",
            "üóëÔ∏è ƒêang qu√©t v√† x√≥a c√°c c√¢u Spam/Intro r√°c...\n",
            "   -> Sau khi l·ªçc Spam: c√≤n 1415 d√≤ng\n",
            "‚úÇÔ∏è ƒêang l·ªçc c√¢u ng·∫Øn (<15 t·ª´)...\n",
            "   -> Sau khi l·ªçc ng·∫Øn: c√≤n 1415 d√≤ng\n",
            "üåç ƒêang ki·ªÉm tra ng√¥n ng·ªØ...\n",
            "ü§ñ ƒêang x√≥a tr√πng l·∫∑p n·ªôi dung (>85%)...\n",
            "\n",
            "========================================\n",
            "‚úÖ HO√ÄN T·∫§T FILE REAL!\n",
            "üëâ C√≤n l·∫°i: 1415 d√≤ng ch·∫•t l∆∞·ª£ng cao.\n",
            "üëâ File l∆∞u t·∫°i: /content/drive/MyDrive/TikTok_Project/dataset_fake_final_clean.csv\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from langdetect import detect, DetectorFactory\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. C·∫§U H√åNH B·ªò L·ªåC\n",
        "# ==============================================================================\n",
        "MIN_CHARS = 70      # ƒê·ªô d√†i t·ªëi thi·ªÉu\n",
        "MIN_WORDS = 15      # S·ªë t·ª´ t·ªëi thi·ªÉu\n",
        "DUPLICATE_THRESHOLD = 0.85 # Ng∆∞·ª°ng tr√πng l·∫∑p\n",
        "\n",
        "# Danh s√°ch t·ª´ kh√≥a \"R√ÅC\" c·∫ßn lo·∫°i b·ªè ngay l·∫≠p t·ª©c\n",
        "SPAM_KEYWORDS = [\n",
        "    \"subscribe\", \"ƒëƒÉng k√Ω k√™nh\", \"b·∫•m chu√¥ng\", \"follow k√™nh\",\n",
        "    \"ghi·ªÅn m√¨ g√µ\", \"ghien mi go\", \"ghienmigo\", \"faptv\", \"Ghi·ªÅn M√¨ G√µ\"\n",
        "    \"copyright\", \"b·∫£n quy·ªÅn\", \"link trong bio\",\n",
        "    \"http://\", \"https://\", \"www.\"\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. C√ÅC H√ÄM X·ª¨ L√ù\n",
        "# ==============================================================================\n",
        "\n",
        "def clean_text_basic(text):\n",
        "    \"\"\"L√†m s·∫°ch s∆° b·ªô k√Ω t·ª± l·∫°\"\"\"\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    # X√≥a k√Ω t·ª± l·∫°, gi·ªØ l·∫°i d·∫•u c√¢u\n",
        "    text = re.sub(r'[^\\w\\s.,!?;:\"\\'\\-%/()]', ' ', text)\n",
        "    # X√≥a kho·∫£ng tr·∫Øng th·ª´a\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def is_spam_content(text):\n",
        "    \"\"\"\n",
        "    Ki·ªÉm tra xem text c√≥ ch·ª©a t·ª´ kh√≥a r√°c ho·∫∑c l√† link URL kh√¥ng.\n",
        "    Tr·∫£ v·ªÅ TRUE n·∫øu l√† SPAM.\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # 1. Check t·ª´ kh√≥a c·∫•m\n",
        "    for keyword in SPAM_KEYWORDS:\n",
        "        if keyword in text_lower:\n",
        "            return True\n",
        "\n",
        "    # 2. Check n·∫øu to√†n b·ªô text nh√¨n gi·ªëng URL\n",
        "    if re.match(r'^[a-zA-Z0-9\\.\\-_]+$', text): # Ki·ªÉu \"vtv.vn\" ho·∫∑c \"tiktok.com\"\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def is_valid_quality(text):\n",
        "    \"\"\"Ki·ªÉm tra ƒë·ªô d√†i v√† ch·∫•t l∆∞·ª£ng n·ªôi dung\"\"\"\n",
        "    # 1. ƒê·ªô d√†i\n",
        "    if len(text) < MIN_CHARS: return False\n",
        "    words = text.split()\n",
        "    if len(words) < MIN_WORDS: return False\n",
        "\n",
        "    # 2. T·ª∑ l·ªá s·ªë (tr√°nh d√≤ng to√†n s·ªë li·ªáu v√¥ nghƒ©a)\n",
        "    digit_count = sum(c.isdigit() for c in text)\n",
        "    if digit_count / len(text) > 0.5: return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def is_vietnamese(text):\n",
        "    \"\"\"Ki·ªÉm tra ng√¥n ng·ªØ\"\"\"\n",
        "    try:\n",
        "        if len(text.split()) < 3: return False\n",
        "        return detect(text) == 'vi'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. H√ÄM CH√çNH: CLEAN FILE REAL\n",
        "# ==============================================================================\n",
        "\n",
        "def clean_real_dataset_specifically():\n",
        "    print(\"üöÄ B·∫ÆT ƒê·∫¶U L√ÄM S·∫†CH DATASET REAL\")\n",
        "\n",
        "    # --- ƒê∆Ø·ªúNG D·∫™N FILE (S·ª≠a l·∫°i t√™n file c·ªßa b·∫°n) ---\n",
        "    # input_csv = \"/content/drive/MyDrive/TikTok_Project/tiktok_videos_all_keywords_real_done.csv\"\n",
        "    # output_csv = \"/content/drive/MyDrive/TikTok_Project/dataset_real_final_clean.csv\"\n",
        "\n",
        "    input_csv = \"/content/drive/MyDrive/TikTok_Project/dataset_fake.csv\"\n",
        "    output_csv = \"/content/drive/MyDrive/TikTok_Project/dataset_fake_final_clean.csv\"\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(input_csv)\n",
        "        print(f\"üìä T·ªïng s·ªë d√≤ng ban ƒë·∫ßu: {len(df)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói ƒë·ªçc file: {e}\")\n",
        "        return\n",
        "\n",
        "    # 1. X√≥a d√≤ng tr·ªëng\n",
        "    df = df.dropna(subset=['text'])\n",
        "    df['text'] = df['text'].astype(str).apply(clean_text_basic)\n",
        "\n",
        "    # 2. L·ªåC SPAM (Subscribe, M√¨ G√µ, URL...)\n",
        "    print(\"üóëÔ∏è ƒêang qu√©t v√† x√≥a c√°c c√¢u Spam/Intro r√°c...\")\n",
        "    # T·∫°o mask nh·ªØng d√≤ng KH√îNG ph·∫£i spam\n",
        "    df['is_clean'] = ~df['text'].apply(is_spam_content)\n",
        "    df = df[df['is_clean'] == True].drop(columns=['is_clean'])\n",
        "    print(f\"   -> Sau khi l·ªçc Spam: c√≤n {len(df)} d√≤ng\")\n",
        "\n",
        "    # 3. L·ªåC CH·∫§T L∆Ø·ª¢NG (ƒê·ªô d√†i)\n",
        "    print(f\"‚úÇÔ∏è ƒêang l·ªçc c√¢u ng·∫Øn (<{MIN_WORDS} t·ª´)...\")\n",
        "    df['is_valid'] = df['text'].apply(is_valid_quality)\n",
        "    df = df[df['is_valid'] == True].drop(columns=['is_valid'])\n",
        "    print(f\"   -> Sau khi l·ªçc ng·∫Øn: c√≤n {len(df)} d√≤ng\")\n",
        "\n",
        "    # 4. L·ªåC TI·∫æNG VI·ªÜT\n",
        "    print(\"üåç ƒêang ki·ªÉm tra ng√¥n ng·ªØ...\")\n",
        "    df['is_vi'] = df['text'].apply(is_vietnamese)\n",
        "    df = df[df['is_vi'] == True].drop(columns=['is_vi'])\n",
        "\n",
        "    # 5. X√ìA TR√ôNG L·∫∂P (FUZZY)\n",
        "    print(\"ü§ñ ƒêang x√≥a tr√πng l·∫∑p n·ªôi dung (>85%)...\")\n",
        "    df = df.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n",
        "\n",
        "    if len(df) > 1:\n",
        "        tfidf = TfidfVectorizer(min_df=1).fit_transform(df['text'])\n",
        "        cosine_sim = cosine_similarity(tfidf, tfidf)\n",
        "        to_drop = set()\n",
        "        for i in range(len(df)):\n",
        "            if i in to_drop: continue\n",
        "            for j in range(i + 1, len(df)):\n",
        "                if j in to_drop: continue\n",
        "                if cosine_sim[i, j] > DUPLICATE_THRESHOLD:\n",
        "                    to_drop.add(j)\n",
        "        df_final = df.drop(index=list(to_drop))\n",
        "    else:\n",
        "        df_final = df\n",
        "\n",
        "    # 6. CHU·∫®N H√ìA CATEGORY\n",
        "    df_final['category'] = 'real'\n",
        "\n",
        "    # --- L∆ØU K·∫æT QU·∫¢ ---\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"‚úÖ HO√ÄN T·∫§T FILE REAL!\")\n",
        "    print(f\"üëâ C√≤n l·∫°i: {len(df_final)} d√≤ng ch·∫•t l∆∞·ª£ng cao.\")\n",
        "    print(f\"üëâ File l∆∞u t·∫°i: {output_csv}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    df_final.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# Ch·∫°y h√†m\n",
        "clean_real_dataset_specifically()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_fake_real_final():\n",
        "    print(\"üöÄ B·∫ÆT ƒê·∫¶U G·ªòP DATASET FAKE V√Ä REAL...\")\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (B·∫°n thay ƒë√∫ng t√™n file c·ªßa b·∫°n v√†o ƒë√¢y)\n",
        "    # ==============================================================================\n",
        "\n",
        "    # File Fake (ƒë√£ c√≥ ƒë·ªß metadata v√† text s·∫°ch)\n",
        "    path_fake = \"/content/drive/MyDrive/TikTok_Project/dataset_fake_final_clean.csv\"\n",
        "\n",
        "    # File Real (ƒë√£ l·ªçc s·∫°ch spam, intro r√°c)\n",
        "    path_real = \"/content/drive/MyDrive/TikTok_Project/dataset_real_final_clean.csv\"\n",
        "\n",
        "    # File k·∫øt qu·∫£ cu·ªëi c√πng (S·∫Ω d√πng file n√†y ƒë·ªÉ Train AI)\n",
        "    path_final = \"/content/drive/MyDrive/TikTok_Project/dataset_TIKTOK_FAKE_NEWS_FINAL.csv\"\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 2. ƒê·ªåC D·ªÆ LI·ªÜU\n",
        "    # ==============================================================================\n",
        "    try:\n",
        "        print(f\"üìÇ ƒêang ƒë·ªçc Fake: {path_fake}\")\n",
        "        df_fake = pd.read_csv(path_fake)\n",
        "        # ƒê·∫£m b·∫£o nh√£n ƒë√∫ng\n",
        "        df_fake['category'] = 'fake'\n",
        "        print(f\"   -> S·ªë l∆∞·ª£ng Fake: {len(df_fake)} d√≤ng\")\n",
        "\n",
        "        print(f\"üìÇ ƒêang ƒë·ªçc Real: {path_real}\")\n",
        "        df_real = pd.read_csv(path_real)\n",
        "        # ƒê·∫£m b·∫£o nh√£n ƒë√∫ng\n",
        "        df_real['category'] = 'real'\n",
        "        print(f\"   -> S·ªë l∆∞·ª£ng Real: {len(df_real)} d√≤ng\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói ƒë·ªçc file: {e}\")\n",
        "        return\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 3. G·ªòP (CONCAT)\n",
        "    # ==============================================================================\n",
        "    print(\"üîó ƒêang gh√©p 2 file l·∫°i v·ªõi nhau...\")\n",
        "\n",
        "    # ignore_index=True ƒë·ªÉ reset l·∫°i s·ªë th·ª© t·ª± t·ª´ 0 ƒë·∫øn h·∫øt\n",
        "    df_merged = pd.concat([df_fake, df_real], ignore_index=True)\n",
        "\n",
        "    print(f\"üìä T·ªïng s·ªë d·ªØ li·ªáu sau khi g·ªôp: {len(df_merged)} d√≤ng\")\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 4. S·∫ÆP X·∫æP C·ªòT (REORDER COLUMNS)\n",
        "    # ==============================================================================\n",
        "    # S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt cho chu·∫©n, ƒë∆∞a c√°c th√¥ng tin quan tr·ªçng l√™n ƒë·∫ßu\n",
        "    # C√°c c·ªôt metadata √≠t quan tr·ªçng h∆°n th√¨ ƒë·ªÉ ra sau\n",
        "\n",
        "    desired_order = [\n",
        "        'url', 'text', 'category',          # 3 c·ªôt quan tr·ªçng nh·∫•t cho Model\n",
        "        'desc', 'keyword', 'createTime',    # Th√¥ng tin n·ªôi dung\n",
        "        'shareCount', 'commentCount', 'playCount', 'diggCount', 'collectCount', # T∆∞∆°ng t√°c\n",
        "        'author_nickname', 'author_unique_id', 'author_id', 'video_id', 'thumnail_url' # Metadata t√°c gi·∫£\n",
        "    ]\n",
        "\n",
        "    # Ch·ªâ l·∫•y nh·ªØng c·ªôt c√≥ trong d·ªØ li·ªáu th·ª±c t·∫ø (ƒë·ªÅ ph√≤ng thi·∫øu c·ªôt n√†o ƒë√≥)\n",
        "    existing_cols = [c for c in desired_order if c in df_merged.columns]\n",
        "\n",
        "    # N·∫øu c√≤n c·ªôt n√†o l·∫° ch∆∞a c√≥ trong danh s√°ch tr√™n th√¨ th√™m v√†o ƒëu√¥i\n",
        "    remaining_cols = [c for c in df_merged.columns if c not in existing_cols]\n",
        "\n",
        "    final_cols = existing_cols + remaining_cols\n",
        "\n",
        "    df_merged = df_merged[final_cols]\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 5. X√ÅO TR·ªòN D·ªÆ LI·ªÜU (SHUFFLE) - T√ôY CH·ªåN\n",
        "    # ==============================================================================\n",
        "    # ƒê·ªÉ khi m·ªü file l√™n kh√¥ng b·ªã ki·ªÉu: 1000 d√≤ng ƒë·∫ßu to√†n Fake, 1000 d√≤ng sau to√†n Real\n",
        "    # Gi√∫p c√°i nh√¨n kh√°ch quan h∆°n\n",
        "    print(\"üîÄ ƒêang x√°o tr·ªôn ng·∫´u nhi√™n th·ª© t·ª± d√≤ng...\")\n",
        "    df_merged = df_merged.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 6. L∆ØU K·∫æT QU·∫¢\n",
        "    # ==============================================================================\n",
        "    df_merged.to_csv(path_final, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üéâ CH√öC M·ª™NG! B·∫†N ƒê√É C√ì DATASET HO√ÄN CH·ªàNH!\")\n",
        "    print(f\"üëâ ƒê∆∞·ªùng d·∫´n: {path_final}\")\n",
        "    print(f\"üëâ T·ªïng s·ªë d√≤ng: {len(df_merged)}\")\n",
        "    print(\"-\" * 30)\n",
        "    print(\"Ph√¢n b·ªë nh√£n:\")\n",
        "    print(df_merged['category'].value_counts())\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Ch·∫°y h√†m\n",
        "merge_fake_real_final()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aVzXvR-i0s1",
        "outputId": "1a71ddd4-c819-40a9-9637-61858a98c39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ B·∫ÆT ƒê·∫¶U G·ªòP DATASET FAKE V√Ä REAL...\n",
            "üìÇ ƒêang ƒë·ªçc Fake: /content/drive/MyDrive/TikTok_Project/dataset_fake_final_clean.csv\n",
            "   -> S·ªë l∆∞·ª£ng Fake: 1415 d√≤ng\n",
            "üìÇ ƒêang ƒë·ªçc Real: /content/drive/MyDrive/TikTok_Project/dataset_real_final_clean.csv\n",
            "   -> S·ªë l∆∞·ª£ng Real: 1069 d√≤ng\n",
            "üîó ƒêang gh√©p 2 file l·∫°i v·ªõi nhau...\n",
            "üìä T·ªïng s·ªë d·ªØ li·ªáu sau khi g·ªôp: 2484 d√≤ng\n",
            "üîÄ ƒêang x√°o tr·ªôn ng·∫´u nhi√™n th·ª© t·ª± d√≤ng...\n",
            "\n",
            "==================================================\n",
            "üéâ CH√öC M·ª™NG! B·∫†N ƒê√É C√ì DATASET HO√ÄN CH·ªàNH!\n",
            "üëâ ƒê∆∞·ªùng d·∫´n: /content/drive/MyDrive/TikTok_Project/dataset_TIKTOK_FAKE_NEWS_FINAL.csv\n",
            "üëâ T·ªïng s·ªë d√≤ng: 2484\n",
            "------------------------------\n",
            "Ph√¢n b·ªë nh√£n:\n",
            "category\n",
            "fake    1415\n",
            "real    1069\n",
            "Name: count, dtype: int64\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}